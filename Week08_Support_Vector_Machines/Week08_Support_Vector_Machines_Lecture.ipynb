{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CE49X: Introduction to Computational Thinking and Data Science for Civil Engineers\n## Week 8: Support Vector Machines\n\n**Instructor:** Dr. Eyuphan Koc  \n**Department of Civil Engineering, Bogazici University**  \n**Semester:** Spring 2026\n\nBased on *Python Data Science Handbook* by Jake VanderPlas  \nChapter 5: Machine Learning (Section 5.07 - Support Vector Machines)  \nhttps://jakevdp.github.io/PythonDataScienceHandbook/\n\n---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to Support Vector Machines](#1.-Introduction-to-Support-Vector-Machines)\n",
    "2. [Maximum Margin Classification](#2.-Maximum-Margin-Classification)\n",
    "3. [Beyond Linear Boundaries: Kernel SVM](#3.-Beyond-Linear-Boundaries:-Kernel-SVM)\n",
    "4. [Tuning the SVM: Softening Margins](#4.-Tuning-the-SVM:-Softening-Margins)\n",
    "5. [Example: Face Recognition](#5.-Example:-Face-Recognition)\n",
    "6. [Support Vector Machine Summary](#6.-Support-Vector-Machine-Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Introduction to Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Support Vector Machines?\n",
    "\n",
    "**Overview:**\n",
    "- Support vector machines (SVMs) are a powerful **discriminative classifier**\n",
    "- Unlike generative models (Naive Bayes), SVMs directly find decision boundaries\n",
    "- Can be used for both **classification** and **regression** tasks\n",
    "- One of the most effective and widely used machine learning algorithms\n",
    "\n",
    "**Key Applications:**\n",
    "- Image classification and face recognition\n",
    "- Text categorization and sentiment analysis\n",
    "- Bioinformatics and medical diagnosis\n",
    "- Structural health monitoring and fault detection\n",
    "\n",
    "> **Key Idea**  \n",
    "> Find the decision boundary that maximizes the margin between classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivating Example: Simple Classification\n",
    "\n",
    "**The Problem:**\n",
    "\n",
    "Consider two classes of points that are well separated in 2D space.\n",
    "\n",
    "**Linear Discriminative Classifier:**\n",
    "- Goal: Draw a straight line separating the two classes\n",
    "- Create a model for classification based on this boundary\n",
    "- For 2D data, this is a line; for higher dimensions, it's a hyperplane\n",
    "\n",
    "**The Challenge:**\n",
    "- There are **many possible dividing lines** that perfectly separate the classes\n",
    "- Each different line would classify new points differently\n",
    "- Simple intuition of \"drawing a line between classes\" is not enough\n",
    "- **Question:** Which line is the best?\n",
    "\n",
    "> **Key Insight: The SVM Solution**  \n",
    "> Among all possible separating lines, choose the one that maximizes the margin to the nearest data points from each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#",
    "#",
    "#",
    " ",
    "[",
    "L",
    "I",
    "V",
    "E",
    "]",
    " ",
    "V",
    "i",
    "s",
    "u",
    "a",
    "l",
    "i",
    "z",
    "i",
    "n",
    "g",
    " ",
    "M",
    "u",
    "l",
    "t",
    "i",
    "p",
    "l",
    "e",
    " ",
    "D",
    "e",
    "c",
    "i",
    "s",
    "i",
    "o",
    "n",
    " ",
    "B",
    "o",
    "u",
    "n",
    "d",
    "a",
    "r",
    "i",
    "e",
    "s",
    "\n",
    "\n",
    "L",
    "e",
    "t",
    " ",
    "u",
    "s",
    " ",
    "g",
    "e",
    "n",
    "e",
    "r",
    "a",
    "t",
    "e",
    " ",
    "t",
    "w",
    "o",
    " ",
    "w",
    "e",
    "l",
    "l",
    "-",
    "s",
    "e",
    "p",
    "a",
    "r",
    "a",
    "t",
    "e",
    "d",
    " ",
    "c",
    "l",
    "a",
    "s",
    "s",
    "e",
    "s",
    " ",
    "a",
    "n",
    "d",
    " ",
    "d",
    "r",
    "a",
    "w",
    " ",
    "s",
    "e",
    "v",
    "e",
    "r",
    "a",
    "l",
    " ",
    "l",
    "i",
    "n",
    "e",
    "s",
    " ",
    "t",
    "h",
    "a",
    "t",
    " ",
    "a",
    "l",
    "l",
    " ",
    "p",
    "e",
    "r",
    "f",
    "e",
    "c",
    "t",
    "l",
    "y",
    " ",
    "s",
    "e",
    "p",
    "a",
    "r",
    "a",
    "t",
    "e",
    " ",
    "t",
    "h",
    "e",
    "m",
    ",",
    " ",
    "i",
    "l",
    "l",
    "u",
    "s",
    "t",
    "r",
    "a",
    "t",
    "i",
    "n",
    "g",
    " ",
    "t",
    "h",
    "e",
    " ",
    "a",
    "m",
    "b",
    "i",
    "g",
    "u",
    "i",
    "t",
    "y",
    " ",
    "t",
    "h",
    "a",
    "t",
    " ",
    "S",
    "V",
    "M",
    " ",
    "r",
    "e",
    "s",
    "o",
    "l",
    "v",
    "e",
    "s",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate sample data with two well-separated clusters\n",
    "X, y = make_blobs(n_samples=50, centers=2,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "\n",
    "# Plot data with three arbitrary separating lines\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "\n",
    "# Draw three plausible separating lines\n",
    "xfit = np.linspace(-1, 3.5)\n",
    "ax.plot(xfit, 1 * xfit + 0.65, '-k', linewidth=1, label='Line 1')\n",
    "ax.plot(xfit, 0.5 * xfit + 1.6, '-k', linewidth=1, linestyle='--', label='Line 2')\n",
    "ax.plot(xfit, -0.2 * xfit + 2.9, '-k', linewidth=1, linestyle='-.', label='Line 3')\n",
    "\n",
    "ax.set_xlim(-1, 3.5)\n",
    "ax.set_ylim(-1, 6)\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_title('Multiple Valid Decision Boundaries')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"All three lines perfectly separate the training data.\")\n",
    "print(\"But they would classify new points differently!\")\n",
    "print(\"Which line should we trust?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Maximum Margin Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Margin Concept\n",
    "\n",
    "**SVM Solution: Maximize the Margin**\n",
    "\n",
    "Rather than drawing a zero-width line, draw a **margin** of some width around each line, up to the nearest point.\n",
    "\n",
    "**Margin Definition:**\n",
    "- The perpendicular distance from the decision boundary to the nearest data point\n",
    "- Creates a \"buffer zone\" around the decision boundary\n",
    "- Larger margin = more confident classification\n",
    "- Points exactly on the margin boundary are called **support vectors**\n",
    "\n",
    "**Maximum Margin Classifier:**\n",
    "- Among all possible separating boundaries, choose the one with the largest margin\n",
    "- This is the **optimal** decision boundary\n",
    "- Provides the most robust classification\n",
    "- Less sensitive to small perturbations in the data\n",
    "\n",
    "> **Key Insight**  \n",
    "> The line that maximizes the margin is the one we choose as the optimal model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#",
    "#",
    "#",
    " ",
    "[",
    "T",
    "O",
    "G",
    "E",
    "T",
    "H",
    "E",
    "R",
    "]",
    " ",
    "V",
    "i",
    "s",
    "u",
    "a",
    "l",
    "i",
    "z",
    "i",
    "n",
    "g",
    " ",
    "M",
    "a",
    "r",
    "g",
    "i",
    "n",
    "s",
    "\n",
    "\n",
    "T",
    "h",
    "e",
    " ",
    "f",
    "o",
    "l",
    "l",
    "o",
    "w",
    "i",
    "n",
    "g",
    " ",
    "v",
    "i",
    "s",
    "u",
    "a",
    "l",
    "i",
    "z",
    "a",
    "t",
    "i",
    "o",
    "n",
    " ",
    "s",
    "h",
    "o",
    "w",
    "s",
    " ",
    "t",
    "h",
    "e",
    " ",
    "o",
    "p",
    "t",
    "i",
    "m",
    "a",
    "l",
    " ",
    "d",
    "e",
    "c",
    "i",
    "s",
    "i",
    "o",
    "n",
    " ",
    "b",
    "o",
    "u",
    "n",
    "d",
    "a",
    "r",
    "y",
    " ",
    "(",
    "s",
    "o",
    "l",
    "i",
    "d",
    " ",
    "l",
    "i",
    "n",
    "e",
    ")",
    ",",
    " ",
    "t",
    "h",
    "e",
    " ",
    "m",
    "a",
    "r",
    "g",
    "i",
    "n",
    " ",
    "b",
    "o",
    "u",
    "n",
    "d",
    "a",
    "r",
    "i",
    "e",
    "s",
    " ",
    "(",
    "d",
    "a",
    "s",
    "h",
    "e",
    "d",
    " ",
    "l",
    "i",
    "n",
    "e",
    "s",
    ")",
    ",",
    " ",
    "a",
    "n",
    "d",
    " ",
    "t",
    "h",
    "e",
    " ",
    "s",
    "u",
    "p",
    "p",
    "o",
    "r",
    "t",
    " ",
    "v",
    "e",
    "c",
    "t",
    "o",
    "r",
    "s",
    " ",
    "(",
    "c",
    "i",
    "r",
    "c",
    "l",
    "e",
    "d",
    " ",
    "p",
    "o",
    "i",
    "n",
    "t",
    "s",
    ")",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for a 2D SVC.\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    # Create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X_grid = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X_grid.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X_grid.shape)\n",
    "\n",
    "    # Plot decision boundary and margins\n",
    "    ax.contour(X_grid, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "\n",
    "    # Plot support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "                   model.support_vectors_[:, 1],\n",
    "                   s=300, linewidth=1, facecolors='none',\n",
    "                   edgecolors='black')\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "X, y = make_blobs(n_samples=50, centers=2,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "\n",
    "# Create and fit the SVM model with a hard margin\n",
    "model = SVC(kernel='linear', C=1E10)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Plot decision boundary with margins\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(model, ax)\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_title('SVM Decision Boundary and Margins')\n",
    "plt.show()\n",
    "\n",
    "print(\"Solid line: optimal decision boundary\")\n",
    "print(\"Dashed lines: margin boundaries\")\n",
    "print(\"Circled points: support vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vectors: The Pivotal Points\n",
    "\n",
    "**What are Support Vectors?**\n",
    "\n",
    "**Definition:**\n",
    "- The training points that lie exactly on the margin boundaries\n",
    "- These are the \"pivotal elements\" of the fit\n",
    "- They give the algorithm its name: **Support Vector Machine**\n",
    "- Only these points determine the decision boundary\n",
    "\n",
    "**Key Property:**\n",
    "- For the fit, **only the position of support vectors matter**\n",
    "- Points far from the margin don't affect the model\n",
    "- Can add/remove non-support-vector points without changing the boundary\n",
    "- This makes SVM very efficient and robust\n",
    "\n",
    "**Mathematical Insight:**\n",
    "- Non-support vectors don't contribute to the loss function\n",
    "- Their position and number don't matter (as long as they're on the correct side)\n",
    "- This is why SVM is **insensitive to outliers** far from the boundary\n",
    "\n",
    "> **Example: Efficiency**  \n",
    "> Even with thousands of training points, often only a handful are support vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the support vectors\n",
    "print(\"Support Vectors:\")\n",
    "print(model.support_vectors_)\n",
    "print(f\"\\nNumber of support vectors: {len(model.support_vectors_)}\")\n",
    "print(f\"Total training points: {len(X)}\")\n",
    "print(f\"Only {len(model.support_vectors_)} out of {len(X)} points define the boundary!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting an SVM in Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC  # \"Support Vector Classifier\"\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate sample data\n",
    "X, y = make_blobs(n_samples=50, centers=2,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "\n",
    "# Create and fit the SVM model\n",
    "model = SVC(kernel='linear', C=1E10)\n",
    "model.fit(X, y)\n",
    "\n",
    "# The support vectors are stored here\n",
    "print(\"Support vector coordinates:\")\n",
    "print(model.support_vectors_)\n",
    "\n",
    "# Make predictions on the data\n",
    "predictions = model.predict(X)\n",
    "print(f\"\\nTraining accuracy: {accuracy_score(y, predictions):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Parameters:**\n",
    "- `kernel='linear'`: Use linear (straight line/plane) decision boundary\n",
    "- `C=1E10`: Very large C means hard margin (we will discuss this later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model Stability\n",
    "\n",
    "**Insensitivity to Distant Points:**\n",
    "\n",
    "A key strength of SVM: the model is determined only by support vectors.\n",
    "\n",
    "**Experiment:**\n",
    "- Train SVM on 60 points from a dataset\n",
    "- Identify the support vectors\n",
    "- Train SVM on 120 points from the same dataset\n",
    "- The same support vectors appear!\n",
    "- The decision boundary remains unchanged\n",
    "\n",
    "**Implications:**\n",
    "- Adding more correctly classified points doesn't change the model\n",
    "- SVM is robust to the exact number of training samples\n",
    "- Focus is on the \"difficult\" points near the boundary\n",
    "- Computationally efficient: only support vectors matter\n",
    "\n",
    "> **Key Insight: Robustness**  \n",
    "> This insensitivity to distant points is one of the key strengths of the SVM model. It focuses on what matters: the boundary region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svm(N=10, ax=None):\n",
    "    \"\"\"Plot SVM with N training points.\"\"\"\n",
    "    X, y = make_blobs(n_samples=200, centers=2,\n",
    "                      random_state=0, cluster_std=0.60)\n",
    "    X = X[:N]\n",
    "    y = y[:N]\n",
    "    model = SVC(kernel='linear', C=1E10)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    ax = ax or plt.gca()\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "    ax.set_xlim(-1, 4)\n",
    "    ax.set_ylim(-1, 6)\n",
    "    plot_svc_decision_function(model, ax)\n",
    "\n",
    "# Compare models with different numbers of points\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "for axi, N in zip(ax, [60, 120]):\n",
    "    plot_svm(N, axi)\n",
    "    axi.set_title(f'N = {N}')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: The support vectors and decision boundary remain the\")\n",
    "print(\"same even when we double the training data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Beyond Linear Boundaries: Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitation of Linear Boundaries\n",
    "\n",
    "**When Linear SVM Fails:**\n",
    "\n",
    "Many real-world datasets are **not linearly separable**.\n",
    "\n",
    "**Example: Concentric Circles**\n",
    "- Inner circle = Class 1\n",
    "- Outer circle = Class 2\n",
    "- No straight line can separate these classes\n",
    "- Linear SVM will perform poorly\n",
    "\n",
    "**The Challenge:**\n",
    "- Real data often has complex, non-linear patterns\n",
    "- Linear boundaries are too restrictive\n",
    "- Need more flexible decision boundaries\n",
    "- But we want to keep SVM's advantages (margin maximization, support vectors)\n",
    "\n",
    "> **Key Insight: Solution -- Kernel Methods**  \n",
    "> Project the data into a higher-dimensional space where it becomes linearly separable, then apply linear SVM in that space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# Generate concentric circles\n",
    "X, y = make_circles(100, factor=.1, noise=.1)\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Non-Linearly Separable Data (Concentric Circles)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Kernel Trick: Intuition\n",
    "\n",
    "**Basic Idea:**\n",
    "\n",
    "Transform the data into a higher-dimensional space where classes become linearly separable.\n",
    "\n",
    "**Example: 2D to 3D Transformation**\n",
    "\n",
    "For concentric circles in 2D $(x, y)$:\n",
    "- Compute radial distance: $r = \\sqrt{x^2 + y^2}$\n",
    "- Project to 3D: $(x, y) \\to (x, y, r)$\n",
    "- Add radial basis function: $r = e^{-(x^2 + y^2)}$\n",
    "- In 3D, classes become linearly separable by a plane\n",
    "\n",
    "**General Strategy:**\n",
    "1. Apply basis function transformation to features\n",
    "2. Create higher-dimensional representation\n",
    "3. Apply linear SVM in transformed space\n",
    "4. Decision boundary is non-linear in original space\n",
    "\n",
    "> **Example: Kernel Functions**  \n",
    "> A kernel function $K(\\mathbf{x}_i, \\mathbf{x}_j)$ computes similarity between data points, implicitly performing this transformation without explicitly constructing the high-dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the kernel transformation: 2D -> 3D\n",
    "# Compute radial basis function for visualization\n",
    "r = np.exp(-(X ** 2).sum(1))\n",
    "\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Original 2D data\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "ax1.set_title('Original 2D Data (NOT linearly separable)')\n",
    "\n",
    "# Projected 3D data\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "ax2.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='autumn')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_zlabel('r = exp(-(x^2 + y^2))')\n",
    "ax2.set_title('Projected 3D Data (linearly separable!)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"In 3D space, the classes become linearly separable!\")\n",
    "print(\"A flat plane can now separate the two groups.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial Basis Function (RBF) Kernel\n",
    "\n",
    "**Most Popular Kernel: RBF (Gaussian Kernel)**\n",
    "\n",
    "**Idea:**\n",
    "- Create a basis function centered at **every** training point\n",
    "- Each basis function is Gaussian: $\\phi_i(\\mathbf{x}) = \\exp\\left(-\\gamma \\|\\mathbf{x} - \\mathbf{x}_i\\|^2\\right)$\n",
    "- Transform $N$ points into $N$ dimensions\n",
    "- Let SVM find the best combination\n",
    "\n",
    "**The Kernel Trick:**\n",
    "- Problem: Projecting $N$ points to $N$ dimensions is expensive!\n",
    "- Solution: Use the **kernel trick**\n",
    "- Compute inner products in high-dimensional space **implicitly**\n",
    "- Never actually construct the $N$-dimensional representation\n",
    "- Makes the computation tractable\n",
    "\n",
    "**RBF Kernel Formula:**\n",
    "\n",
    "$$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left(-\\gamma \\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2\\right)$$\n",
    "\n",
    "where $\\gamma$ controls the \"reach\" of each training point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#",
    "#",
    "#",
    " ",
    "[",
    "L",
    "I",
    "V",
    "E",
    "]",
    " ",
    "K",
    "e",
    "r",
    "n",
    "e",
    "l",
    " ",
    "S",
    "V",
    "M",
    " ",
    "V",
    "i",
    "s",
    "u",
    "a",
    "l",
    "i",
    "z",
    "a",
    "t",
    "i",
    "o",
    "n",
    ":",
    " ",
    "L",
    "i",
    "n",
    "e",
    "a",
    "r",
    " ",
    "v",
    "s",
    ".",
    " ",
    "R",
    "B",
    "F",
    "\n",
    "\n",
    "L",
    "e",
    "t",
    " ",
    "u",
    "s",
    " ",
    "c",
    "o",
    "m",
    "p",
    "a",
    "r",
    "e",
    " ",
    "a",
    " ",
    "l",
    "i",
    "n",
    "e",
    "a",
    "r",
    " ",
    "k",
    "e",
    "r",
    "n",
    "e",
    "l",
    " ",
    "(",
    "w",
    "h",
    "i",
    "c",
    "h",
    " ",
    "f",
    "a",
    "i",
    "l",
    "s",
    " ",
    "o",
    "n",
    " ",
    "c",
    "o",
    "n",
    "c",
    "e",
    "n",
    "t",
    "r",
    "i",
    "c",
    " ",
    "c",
    "i",
    "r",
    "c",
    "l",
    "e",
    "s",
    ")",
    " ",
    "w",
    "i",
    "t",
    "h",
    " ",
    "t",
    "h",
    "e",
    " ",
    "R",
    "B",
    "F",
    " ",
    "k",
    "e",
    "r",
    "n",
    "e",
    "l",
    " ",
    "(",
    "w",
    "h",
    "i",
    "c",
    "h",
    " ",
    "s",
    "u",
    "c",
    "c",
    "e",
    "e",
    "d",
    "s",
    ")",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate concentric circles\n",
    "X, y = make_circles(100, factor=.1, noise=.1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left: Linear kernel (fails)\n",
    "clf_linear = SVC(kernel='linear').fit(X, y)\n",
    "axes[0].scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(clf_linear, axes[0], plot_support=False)\n",
    "axes[0].set_title(f'Linear Kernel (Accuracy: {clf_linear.score(X, y):.2%})')\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "\n",
    "# Right: RBF kernel (succeeds)\n",
    "clf_rbf = SVC(kernel='rbf', C=1E6).fit(X, y)\n",
    "axes[1].scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(clf_rbf, axes[1])\n",
    "axes[1].set_title(f'RBF Kernel (Accuracy: {clf_rbf.score(X, y):.2%})')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Left: Linear kernel CANNOT separate concentric circles.\")\n",
    "print(\"Right: RBF kernel creates a circular decision boundary and succeeds!\")\n",
    "print(\"Non-linear boundaries allow SVM to handle complex patterns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Kernel Functions\n",
    "\n",
    "Scikit-Learn provides several kernels:\n",
    "\n",
    "| Kernel | Formula | Use Case |\n",
    "|--------|---------|----------|\n",
    "| Linear | $K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i \\cdot \\mathbf{x}_j$ | Linearly separable data |\n",
    "| Polynomial | $K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\gamma\\, \\mathbf{x}_i \\cdot \\mathbf{x}_j + r)^d$ | Polynomial relationships |\n",
    "| RBF (Gaussian) | $K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma \\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2)$ | General purpose, most popular |\n",
    "| Sigmoid | $K(\\mathbf{x}_i, \\mathbf{x}_j) = \\tanh(\\gamma\\, \\mathbf{x}_i \\cdot \\mathbf{x}_j + r)$ | Similar to neural networks |\n",
    "\n",
    "**Choosing a Kernel:**\n",
    "- **Linear:** Start here for high-dimensional data, fast and interpretable\n",
    "- **RBF:** Default choice for non-linear problems, works well in most cases\n",
    "- **Polynomial:** When you expect polynomial relationships, but can be slow\n",
    "- **Custom:** You can define your own kernel function for specialized applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all kernel types on linearly separable data\n",
    "X_blobs, y_blobs = make_blobs(n_samples=100, centers=2,\n",
    "                              random_state=42, cluster_std=1.0)\n",
    "\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, kernel in enumerate(kernels):\n",
    "    model = SVC(kernel=kernel, C=1.0, gamma='auto')\n",
    "    model.fit(X_blobs, y_blobs)\n",
    "\n",
    "    axes[idx].scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_blobs, s=50, cmap='autumn')\n",
    "    plot_svc_decision_function(model, axes[idx])\n",
    "    axes[idx].set_title(f'{kernel.upper()} Kernel (Accuracy: {model.score(X_blobs, y_blobs):.2%})')\n",
    "    axes[idx].set_xlabel('Feature 1')\n",
    "    axes[idx].set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Tuning the SVM: Softening Margins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem with Hard Margins\n",
    "\n",
    "**Real Data is Messy:**\n",
    "\n",
    "Our discussion so far assumed perfectly separable data. But what if:\n",
    "- Classes overlap slightly?\n",
    "- There are outliers?\n",
    "- No perfect separation exists?\n",
    "\n",
    "**Hard Margin SVM:**\n",
    "- Requires all points to be on the correct side of the margin\n",
    "- Cannot handle any misclassification\n",
    "- Very sensitive to outliers\n",
    "- May not find a solution if data is not separable\n",
    "\n",
    "**Solution: Soft Margin SVM**\n",
    "- Allow some points to be **within** or even **on the wrong side** of the margin\n",
    "- Trade-off between: (1) wide margin, (2) few misclassifications\n",
    "- Controlled by a parameter $C$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The C Parameter\n",
    "\n",
    "**Tuning Parameter: C**\n",
    "\n",
    "The $C$ parameter controls the hardness of the margin:\n",
    "\n",
    "**Large C (e.g., $C = 10^{10}$):**\n",
    "- Hard margin\n",
    "- Penalizes misclassifications heavily\n",
    "- Narrow margin, tries to classify all training points correctly\n",
    "- Risk of overfitting\n",
    "- Sensitive to outliers\n",
    "\n",
    "**Small C (e.g., $C = 0.1$):**\n",
    "- Soft margin\n",
    "- Tolerates some misclassifications\n",
    "- Wide margin, better generalization\n",
    "- Risk of underfitting\n",
    "- Robust to outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#",
    "#",
    "#",
    " ",
    "[",
    "L",
    "I",
    "V",
    "E",
    "]",
    " ",
    "S",
    "o",
    "f",
    "t",
    " ",
    "M",
    "a",
    "r",
    "g",
    "i",
    "n",
    " ",
    "V",
    "i",
    "s",
    "u",
    "a",
    "l",
    "i",
    "z",
    "a",
    "t",
    "i",
    "o",
    "n",
    "\n",
    "\n",
    "C",
    "o",
    "m",
    "p",
    "a",
    "r",
    "e",
    " ",
    "a",
    " ",
    "l",
    "a",
    "r",
    "g",
    "e",
    " ",
    "C",
    " ",
    "(",
    "h",
    "a",
    "r",
    "d",
    " ",
    "m",
    "a",
    "r",
    "g",
    "i",
    "n",
    ",",
    " ",
    "n",
    "a",
    "r",
    "r",
    "o",
    "w",
    ",",
    " ",
    "c",
    "o",
    "m",
    "p",
    "l",
    "e",
    "x",
    " ",
    "b",
    "o",
    "u",
    "n",
    "d",
    "a",
    "r",
    "y",
    ")",
    " ",
    "w",
    "i",
    "t",
    "h",
    " ",
    "a",
    " ",
    "s",
    "m",
    "a",
    "l",
    "l",
    " ",
    "C",
    " ",
    "(",
    "s",
    "o",
    "f",
    "t",
    " ",
    "m",
    "a",
    "r",
    "g",
    "i",
    "n",
    ",",
    " ",
    "w",
    "i",
    "d",
    "e",
    ",",
    " ",
    "s",
    "i",
    "m",
    "p",
    "l",
    "e",
    " ",
    "b",
    "o",
    "u",
    "n",
    "d",
    "a",
    "r",
    "y",
    ")",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with some overlap\n",
    "X, y = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=0, cluster_std=0.8)\n",
    "\n",
    "# Compare different C values\n",
    "C_values = [10.0, 0.1]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "for axi, C in zip(ax, C_values):\n",
    "    model = SVC(kernel='linear', C=C).fit(X, y)\n",
    "    axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "    plot_svc_decision_function(model, axi)\n",
    "    axi.scatter(model.support_vectors_[:, 0],\n",
    "                model.support_vectors_[:, 1],\n",
    "                s=300, lw=1, facecolors='none', edgecolors='black')\n",
    "    axi.set_title(f'C = {C:.1f}', size=14)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Left (C=10.0): Hard margin -- narrow margin, tries to classify all points correctly\")\n",
    "print(\"Right (C=0.1): Soft margin -- wide margin, accepts some misclassifications\")\n",
    "print(\"\\nSoft margin often generalizes better to new, unseen data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Optimal C\n",
    "\n",
    "- Use cross-validation to tune $C$\n",
    "- Try logarithmic range: $[10^{-3}, 10^{-2}, 10^{-1}, 1, 10, 100, 1000]$\n",
    "- Balance between training accuracy and generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning Strategy\n",
    "\n",
    "**Two Main Hyperparameters to Tune:**\n",
    "\n",
    "**1. Kernel Parameter ($\\gamma$ for RBF kernel):**\n",
    "- Controls the \"reach\" of each training point\n",
    "- Large $\\gamma$: tight fit, each point has small influence\n",
    "- Small $\\gamma$: loose fit, each point has wide influence\n",
    "- Typical range: $[10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1]$\n",
    "\n",
    "**2. Regularization Parameter ($C$):**\n",
    "- Controls margin hardness\n",
    "- Large $C$: hard margin, fewer errors on training data\n",
    "- Small $C$: soft margin, wider margin\n",
    "- Typical range: $[10^{-2}, 10^{-1}, 1, 10, 10^2, 10^3]$\n",
    "\n",
    "**Tuning Approach:**\n",
    "- Use **grid search** with cross-validation\n",
    "- Try combinations of both parameters\n",
    "- Select the combination with best cross-validation score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#",
    "#",
    "#",
    " ",
    "[",
    "P",
    "R",
    "A",
    "C",
    "T",
    "I",
    "C",
    "E",
    "]",
    " ",
    "E",
    "f",
    "f",
    "e",
    "c",
    "t",
    " ",
    "o",
    "f",
    " ",
    "G",
    "a",
    "m",
    "m",
    "a",
    " ",
    "P",
    "a",
    "r",
    "a",
    "m",
    "e",
    "t",
    "e",
    "r",
    " ",
    "(",
    "R",
    "B",
    "F",
    " ",
    "K",
    "e",
    "r",
    "n",
    "e",
    "l",
    ")",
    "\n",
    "\n",
    "G",
    "a",
    "m",
    "m",
    "a",
    " ",
    "c",
    "o",
    "n",
    "t",
    "r",
    "o",
    "l",
    "s",
    " ",
    "t",
    "h",
    "e",
    " ",
    "\"",
    "r",
    "e",
    "a",
    "c",
    "h",
    "\"",
    " ",
    "o",
    "f",
    " ",
    "e",
    "a",
    "c",
    "h",
    " ",
    "t",
    "r",
    "a",
    "i",
    "n",
    "i",
    "n",
    "g",
    " ",
    "p",
    "o",
    "i",
    "n",
    "t",
    ":",
    "\n",
    "-",
    " ",
    "L",
    "a",
    "r",
    "g",
    "e",
    " ",
    "g",
    "a",
    "m",
    "m",
    "a",
    " ",
    "=",
    " ",
    "t",
    "i",
    "g",
    "h",
    "t",
    " ",
    "f",
    "i",
    "t",
    ",",
    " ",
    "s",
    "m",
    "a",
    "l",
    "l",
    " ",
    "i",
    "n",
    "f",
    "l",
    "u",
    "e",
    "n",
    "c",
    "e",
    " ",
    "r",
    "e",
    "g",
    "i",
    "o",
    "n",
    "\n",
    "-",
    " ",
    "S",
    "m",
    "a",
    "l",
    "l",
    " ",
    "g",
    "a",
    "m",
    "m",
    "a",
    " ",
    "=",
    " ",
    "l",
    "o",
    "o",
    "s",
    "e",
    " ",
    "f",
    "i",
    "t",
    ",",
    " ",
    "l",
    "a",
    "r",
    "g",
    "e",
    " ",
    "i",
    "n",
    "f",
    "l",
    "u",
    "e",
    "n",
    "c",
    "e",
    " ",
    "r",
    "e",
    "g",
    "i",
    "o",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate non-linear data\n",
    "X_circles, y_circles = make_circles(100, factor=.1, noise=.1)\n",
    "\n",
    "# Test different gamma values\n",
    "gamma_values = [0.1, 1.0, 10.0, 100.0]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, gamma in enumerate(gamma_values):\n",
    "    model = SVC(kernel='rbf', C=1.0, gamma=gamma)\n",
    "    model.fit(X_circles, y_circles)\n",
    "\n",
    "    axes[idx].scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles, s=50, cmap='autumn')\n",
    "    plot_svc_decision_function(model, axes[idx])\n",
    "    axes[idx].set_title(f'Gamma = {gamma} (Accuracy: {model.score(X_circles, y_circles):.2%})')\n",
    "    axes[idx].set_xlabel('Feature 1')\n",
    "    axes[idx].set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: Very large gamma can lead to overfitting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Example: Face Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#",
    "#",
    "#",
    " ",
    "[",
    "T",
    "O",
    "G",
    "E",
    "T",
    "H",
    "E",
    "R",
    "]",
    " ",
    "A",
    "p",
    "p",
    "l",
    "i",
    "c",
    "a",
    "t",
    "i",
    "o",
    "n",
    ":",
    " ",
    "F",
    "a",
    "c",
    "e",
    " ",
    "R",
    "e",
    "c",
    "o",
    "g",
    "n",
    "i",
    "t",
    "i",
    "o",
    "n",
    "\n",
    "\n",
    "*",
    "*",
    "R",
    "e",
    "a",
    "l",
    "-",
    "W",
    "o",
    "r",
    "l",
    "d",
    " ",
    "E",
    "x",
    "a",
    "m",
    "p",
    "l",
    "e",
    ":",
    "*",
    "*",
    "\n",
    "\n",
    "U",
    "s",
    "e",
    " ",
    "S",
    "V",
    "M",
    " ",
    "t",
    "o",
    " ",
    "r",
    "e",
    "c",
    "o",
    "g",
    "n",
    "i",
    "z",
    "e",
    " ",
    "f",
    "a",
    "c",
    "e",
    "s",
    " ",
    "f",
    "r",
    "o",
    "m",
    " ",
    "i",
    "m",
    "a",
    "g",
    "e",
    "s",
    ".",
    "\n",
    "\n",
    "*",
    "*",
    "D",
    "a",
    "t",
    "a",
    "s",
    "e",
    "t",
    ":",
    " ",
    "L",
    "a",
    "b",
    "e",
    "l",
    "e",
    "d",
    " ",
    "F",
    "a",
    "c",
    "e",
    "s",
    " ",
    "i",
    "n",
    " ",
    "t",
    "h",
    "e",
    " ",
    "W",
    "i",
    "l",
    "d",
    "*",
    "*",
    "\n",
    "-",
    " ",
    "S",
    "e",
    "v",
    "e",
    "r",
    "a",
    "l",
    " ",
    "t",
    "h",
    "o",
    "u",
    "s",
    "a",
    "n",
    "d",
    " ",
    "p",
    "h",
    "o",
    "t",
    "o",
    "s",
    " ",
    "o",
    "f",
    " ",
    "p",
    "u",
    "b",
    "l",
    "i",
    "c",
    " ",
    "f",
    "i",
    "g",
    "u",
    "r",
    "e",
    "s",
    "\n",
    "-",
    " ",
    "M",
    "u",
    "l",
    "t",
    "i",
    "p",
    "l",
    "e",
    " ",
    "p",
    "h",
    "o",
    "t",
    "o",
    "s",
    " ",
    "p",
    "e",
    "r",
    " ",
    "p",
    "e",
    "r",
    "s",
    "o",
    "n",
    "\n",
    "-",
    " ",
    "B",
    "u",
    "i",
    "l",
    "t",
    "-",
    "i",
    "n",
    " ",
    "f",
    "e",
    "t",
    "c",
    "h",
    "e",
    "r",
    " ",
    "i",
    "n",
    " ",
    "S",
    "c",
    "i",
    "k",
    "i",
    "t",
    "-",
    "L",
    "e",
    "a",
    "r",
    "n",
    "\n",
    "-",
    " ",
    "S",
    "e",
    "l",
    "e",
    "c",
    "t",
    " ",
    "p",
    "e",
    "o",
    "p",
    "l",
    "e",
    " ",
    "w",
    "i",
    "t",
    "h",
    " ",
    "a",
    "t",
    " ",
    "l",
    "e",
    "a",
    "s",
    "t",
    " ",
    "6",
    "0",
    " ",
    "p",
    "h",
    "o",
    "t",
    "o",
    "s",
    " ",
    "f",
    "o",
    "r",
    " ",
    "s",
    "u",
    "f",
    "f",
    "i",
    "c",
    "i",
    "e",
    "n",
    "t",
    " ",
    "t",
    "r",
    "a",
    "i",
    "n",
    "i",
    "n",
    "g",
    " ",
    "d",
    "a",
    "t",
    "a",
    "\n",
    "\n",
    "*",
    "*",
    "T",
    "o",
    "t",
    "a",
    "l",
    ":",
    "*",
    "*",
    " ",
    "8",
    " ",
    "c",
    "l",
    "a",
    "s",
    "s",
    "e",
    "s",
    ",",
    " ",
    "1",
    ",",
    "3",
    "4",
    "8",
    " ",
    "i",
    "m",
    "a",
    "g",
    "e",
    "s",
    " ",
    "o",
    "f",
    " ",
    "s",
    "i",
    "z",
    "e",
    " ",
    "$",
    "6",
    "2",
    " ",
    "\\",
    "t",
    "i",
    "m",
    "e",
    "s",
    " ",
    "4",
    "7",
    "$",
    " ",
    "p",
    "i",
    "x",
    "e",
    "l",
    "s",
    " ",
    "(",
    "2",
    ",",
    "9",
    "1",
    "4",
    " ",
    "f",
    "e",
    "a",
    "t",
    "u",
    "r",
    "e",
    "s",
    " ",
    "p",
    "e",
    "r",
    " ",
    "i",
    "m",
    "a",
    "g",
    "e",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "# Fetch faces with at least 60 photos per person\n",
    "faces = fetch_lfw_people(min_faces_per_person=60)\n",
    "\n",
    "print(\"Categories (people):\")\n",
    "print(faces.target_names)\n",
    "print(f\"\\nDataset shape: {faces.images.shape}\")\n",
    "print(f\"Number of images: {faces.images.shape[0]}\")\n",
    "print(f\"Image size: {faces.images.shape[1]} x {faces.images.shape[2]}\")\n",
    "print(f\"Total features per image: {faces.images.shape[1] * faces.images.shape[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Sample Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sample faces\n",
    "fig, ax = plt.subplots(3, 5, figsize=(12, 8))\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(faces.images[i], cmap='bone')\n",
    "    axi.set(xticks=[], yticks=[],\n",
    "            xlabel=faces.target_names[faces.target[i]])\n",
    "plt.suptitle('Sample Faces from Dataset', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge: High-Dimensional Data\n",
    "\n",
    "**The Dimensionality Problem:**\n",
    "- Each image has $62 \\times 47 = 2{,}914$ pixels\n",
    "- Could use each pixel as a feature\n",
    "- But raw pixels are not the best features\n",
    "- Too many dimensions can cause overfitting\n",
    "- Need feature extraction/dimensionality reduction\n",
    "\n",
    "**Solution: Principal Component Analysis (PCA)**\n",
    "- Extract the most important patterns (principal components)\n",
    "- Reduce 2,914 dimensions to 150 dimensions\n",
    "- Retain most of the information\n",
    "- More meaningful features for classification\n",
    "\n",
    "> **Example: Pipeline Approach**  \n",
    "> Combine PCA (preprocessing) and SVM (classification) into a single pipeline for clean, efficient code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Face Recognition Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create pipeline: PCA (dimension reduction) -> SVM (classification)\n",
    "pca = PCA(n_components=150, whiten=True, random_state=42)\n",
    "svc = SVC(kernel='rbf', class_weight='balanced')\n",
    "model = make_pipeline(pca, svc)\n",
    "\n",
    "print(\"Pipeline created: PCA(150 components) -> SVM(RBF kernel)\")\n",
    "print(\"Pipeline benefits: Preprocessing and model fit in one step, prevents data leakage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(\n",
    "    faces.data, faces.target, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {Xtrain.shape}\")\n",
    "print(f\"Test set size: {Xtest.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search for Hyperparameter Tuning\n",
    "\n",
    "Find optimal values for C and gamma using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'svc__C': [1, 5, 10, 50],\n",
    "    'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid = GridSearchCV(model, param_grid, cv=5)\n",
    "\n",
    "print(\"Starting grid search...\")\n",
    "print(f\"Testing {len(param_grid['svc__C']) * len(param_grid['svc__gamma'])} parameter combinations\")\n",
    "print(\"This may take a minute...\\n\")\n",
    "\n",
    "%time grid.fit(Xtrain, ytrain)\n",
    "\n",
    "print(f\"\\nBest parameters: {grid.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid.best_score_:.2%}\")\n",
    "\n",
    "# Note: Grid search tries all 16 combinations (4 x 4) with\n",
    "# cross-validation to find the best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Face Recognition Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best model for predictions\n",
    "best_model = grid.best_estimator_\n",
    "yfit = best_model.predict(Xtest)\n",
    "\n",
    "print(f\"Test set accuracy: {best_model.score(Xtest, ytest):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample predictions\n",
    "fig, ax = plt.subplots(4, 6, figsize=(12, 8))\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(Xtest[i].reshape(62, 47), cmap='bone')\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    # Color correct predictions in black, wrong predictions in red\n",
    "    color = 'black' if yfit[i] == ytest[i] else 'red'\n",
    "    axi.set_ylabel(faces.target_names[yfit[i]].split()[-1],\n",
    "                   color=color)\n",
    "plt.suptitle('Predicted Names (Incorrect Labels in Red)', size=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(\"=\" * 70)\n",
    "print(classification_report(ytest, yfit,\n",
    "                            target_names=faces.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "- **Precision:** Of the faces predicted as person X, what fraction are correct?\n",
    "- **Recall:** Of all faces of person X, what fraction did we find?\n",
    "- **F1-Score:** Harmonic mean of precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix for Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Compute confusion matrix\n",
    "mat = confusion_matrix(ytest, yfit)\n",
    "\n",
    "# Plot confusion matrix as heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=True,\n",
    "            xticklabels=faces.target_names,\n",
    "            yticklabels=faces.target_names,\n",
    "            cmap='Blues')\n",
    "plt.xlabel('True Label')\n",
    "plt.ylabel('Predicted Label')\n",
    "plt.title('Confusion Matrix for Face Recognition')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Observations:\")\n",
    "print(\"- Diagonal values = correct predictions\")\n",
    "print(\"- Off-diagonal values = misclassifications\")\n",
    "print(\"- Darker blue = more predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations and Modern Approaches\n",
    "\n",
    "**Limitations of This Example:**\n",
    "- Dataset has pre-cropped faces; real-world needs face detection first\n",
    "- Controlled conditions (professional photos); less variation\n",
    "- Limited training data (60-530 images per person)\n",
    "\n",
    "**Modern Approaches:**\n",
    "- Use CNNs instead of PCA+SVM\n",
    "- Learn features automatically from raw pixels\n",
    "- State-of-the-art: 99%+ accuracy on face recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Support Vector Machine Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Advantages\n",
    "\n",
    "**Why Use Support Vector Machines?**\n",
    "\n",
    "**1. Effective in High Dimensions**\n",
    "- Work well even when dimensions exceed samples (e.g., face recognition: 2,914 features, 1,011 samples)\n",
    "\n",
    "**2. Memory Efficient**\n",
    "- Model defined by support vectors only (small fraction of training points)\n",
    "- Fast prediction even with large training sets\n",
    "\n",
    "**3. Versatile**\n",
    "- Different kernels for different data types; can model complex non-linear boundaries\n",
    "\n",
    "**4. Robust**\n",
    "- Insensitive to points far from boundary; less affected by outliers\n",
    "\n",
    "**5. Mathematically Elegant**\n",
    "- Convex optimization with unique global solution\n",
    "\n",
    "**6. Often High Accuracy**\n",
    "- Especially strong on medium-sized datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Disadvantages\n",
    "\n",
    "**When NOT to Use SVM:**\n",
    "\n",
    "**1. Computational Cost**\n",
    "- Training time: $\\mathcal{O}(N^2)$ to $\\mathcal{O}(N^3)$ -- slow on large datasets ($N > 10{,}000$)\n",
    "- Grid search for hyperparameters is expensive\n",
    "\n",
    "**2. Hyperparameter Sensitivity**\n",
    "- Results depend heavily on $C$ and $\\gamma$ -- requires careful cross-validation\n",
    "\n",
    "**3. No Direct Probability Estimates**\n",
    "- SVM outputs class labels; probability estimates require extra calibration\n",
    "\n",
    "**4. Black Box Nature**\n",
    "- With kernel methods, less interpretable than linear models\n",
    "- Hard to understand why a particular classification was made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use SVM\n",
    "\n",
    "**SVM is a Good Choice When:**\n",
    "- Medium-sized dataset (hundreds to thousands of samples)\n",
    "- High-dimensional data (many features)\n",
    "- Clear margin of separation between classes\n",
    "- Accuracy is more important than speed/interpretability\n",
    "- Willing to invest time in hyperparameter tuning\n",
    "\n",
    "**Consider Alternatives When:**\n",
    "- Very large dataset ($N > 100{,}000$) --> use linear models, SGD, or deep learning\n",
    "- Need probability estimates --> use logistic regression, Naive Bayes\n",
    "- Need interpretability --> use decision trees, linear models\n",
    "- Very small dataset --> try simpler models first (Naive Bayes, logistic regression)\n",
    "- Real-time predictions required --> simpler models may be faster\n",
    "\n",
    "> **Key Insight: Author's Recommendation**  \n",
    "> \"I generally only turn to SVMs once other simpler, faster, and less tuning-intensive methods have been shown to be insufficient for my needs.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways and Best Practices\n",
    "\n",
    "**Core Concepts:**\n",
    "- **Maximum Margin**: Choose boundary with largest margin\n",
    "- **Support Vectors**: Only points on margin matter (compact model)\n",
    "- **Kernel Trick**: Non-linear boundaries without explicit transformation\n",
    "- **Soft Margins**: Parameter $C$ controls margin hardness\n",
    "\n",
    "**Best Practices:**\n",
    "- **Always** scale features (`StandardScaler`)\n",
    "- Start with linear kernel, then try RBF if needed\n",
    "- Use `GridSearchCV`: $C \\in [10^{-2}, 10^3]$, $\\gamma \\in [10^{-4}, 1]$\n",
    "- For high-dimensional data, consider PCA first\n",
    "\n",
    "> **Key Insight: Final Thought**  \n",
    "> SVMs excel on medium-sized, high-dimensional datasets. Try simpler models first; use SVM when you need the extra accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-",
    "-",
    "-",
    "\n",
    "\n",
    "#",
    "#",
    " ",
    "[",
    "P",
    "R",
    "A",
    "C",
    "T",
    "I",
    "C",
    "E",
    "]",
    " ",
    "P",
    "r",
    "a",
    "c",
    "t",
    "i",
    "c",
    "e",
    " ",
    "E",
    "x",
    "e",
    "r",
    "c",
    "i",
    "s",
    "e",
    "s",
    "\n",
    "\n",
    "T",
    "r",
    "y",
    " ",
    "t",
    "h",
    "e",
    "s",
    "e",
    " ",
    "o",
    "n",
    " ",
    "y",
    "o",
    "u",
    "r",
    " ",
    "o",
    "w",
    "n",
    ":",
    "\n",
    "\n",
    "1",
    ".",
    " ",
    "*",
    "*",
    "E",
    "x",
    "e",
    "r",
    "c",
    "i",
    "s",
    "e",
    " ",
    "1",
    ":",
    "*",
    "*",
    " ",
    "C",
    "r",
    "e",
    "a",
    "t",
    "e",
    " ",
    "y",
    "o",
    "u",
    "r",
    " ",
    "o",
    "w",
    "n",
    " ",
    "2",
    "D",
    " ",
    "d",
    "a",
    "t",
    "a",
    "s",
    "e",
    "t",
    " ",
    "w",
    "i",
    "t",
    "h",
    " ",
    "`",
    "m",
    "a",
    "k",
    "e",
    "_",
    "b",
    "l",
    "o",
    "b",
    "s",
    "`",
    " ",
    "a",
    "n",
    "d",
    " ",
    "e",
    "x",
    "p",
    "e",
    "r",
    "i",
    "m",
    "e",
    "n",
    "t",
    " ",
    "w",
    "i",
    "t",
    "h",
    " ",
    "d",
    "i",
    "f",
    "f",
    "e",
    "r",
    "e",
    "n",
    "t",
    " ",
    "C",
    " ",
    "v",
    "a",
    "l",
    "u",
    "e",
    "s",
    ".",
    " ",
    "W",
    "h",
    "a",
    "t",
    " ",
    "h",
    "a",
    "p",
    "p",
    "e",
    "n",
    "s",
    " ",
    "w",
    "h",
    "e",
    "n",
    " ",
    "C",
    " ",
    "i",
    "s",
    " ",
    "v",
    "e",
    "r",
    "y",
    " ",
    "s",
    "m",
    "a",
    "l",
    "l",
    " ",
    "(",
    "0",
    ".",
    "0",
    "1",
    ")",
    " ",
    "o",
    "r",
    " ",
    "v",
    "e",
    "r",
    "y",
    " ",
    "l",
    "a",
    "r",
    "g",
    "e",
    " ",
    "(",
    "1",
    "0",
    "0",
    "0",
    ")",
    "?",
    "\n",
    "\n",
    "2",
    ".",
    " ",
    "*",
    "*",
    "E",
    "x",
    "e",
    "r",
    "c",
    "i",
    "s",
    "e",
    " ",
    "2",
    ":",
    "*",
    "*",
    " ",
    "G",
    "e",
    "n",
    "e",
    "r",
    "a",
    "t",
    "e",
    " ",
    "a",
    " ",
    "m",
    "o",
    "r",
    "e",
    " ",
    "c",
    "o",
    "m",
    "p",
    "l",
    "e",
    "x",
    " ",
    "n",
    "o",
    "n",
    "-",
    "l",
    "i",
    "n",
    "e",
    "a",
    "r",
    " ",
    "d",
    "a",
    "t",
    "a",
    "s",
    "e",
    "t",
    " ",
    "u",
    "s",
    "i",
    "n",
    "g",
    " ",
    "`",
    "m",
    "a",
    "k",
    "e",
    "_",
    "m",
    "o",
    "o",
    "n",
    "s",
    "`",
    " ",
    "f",
    "r",
    "o",
    "m",
    " ",
    "s",
    "k",
    "l",
    "e",
    "a",
    "r",
    "n",
    ".",
    " ",
    "C",
    "o",
    "m",
    "p",
    "a",
    "r",
    "e",
    " ",
    "l",
    "i",
    "n",
    "e",
    "a",
    "r",
    " ",
    "a",
    "n",
    "d",
    " ",
    "R",
    "B",
    "F",
    " ",
    "k",
    "e",
    "r",
    "n",
    "e",
    "l",
    "s",
    ".",
    "\n",
    "\n",
    "3",
    ".",
    " ",
    "*",
    "*",
    "E",
    "x",
    "e",
    "r",
    "c",
    "i",
    "s",
    "e",
    " ",
    "3",
    ":",
    "*",
    "*",
    " ",
    "L",
    "o",
    "a",
    "d",
    " ",
    "a",
    " ",
    "d",
    "i",
    "f",
    "f",
    "e",
    "r",
    "e",
    "n",
    "t",
    " ",
    "d",
    "a",
    "t",
    "a",
    "s",
    "e",
    "t",
    " ",
    "(",
    "e",
    ".",
    "g",
    ".",
    ",",
    " ",
    "d",
    "i",
    "g",
    "i",
    "t",
    "s",
    " ",
    "d",
    "a",
    "t",
    "a",
    "s",
    "e",
    "t",
    ")",
    " ",
    "a",
    "n",
    "d",
    " ",
    "b",
    "u",
    "i",
    "l",
    "d",
    " ",
    "a",
    "n",
    " ",
    "S",
    "V",
    "M",
    " ",
    "c",
    "l",
    "a",
    "s",
    "s",
    "i",
    "f",
    "i",
    "e",
    "r",
    ".",
    " ",
    "U",
    "s",
    "e",
    " ",
    "g",
    "r",
    "i",
    "d",
    " ",
    "s",
    "e",
    "a",
    "r",
    "c",
    "h",
    " ",
    "t",
    "o",
    " ",
    "f",
    "i",
    "n",
    "d",
    " ",
    "o",
    "p",
    "t",
    "i",
    "m",
    "a",
    "l",
    " ",
    "h",
    "y",
    "p",
    "e",
    "r",
    "p",
    "a",
    "r",
    "a",
    "m",
    "e",
    "t",
    "e",
    "r",
    "s",
    ".",
    "\n",
    "\n",
    "4",
    ".",
    " ",
    "*",
    "*",
    "E",
    "x",
    "e",
    "r",
    "c",
    "i",
    "s",
    "e",
    " ",
    "4",
    ":",
    "*",
    "*",
    " ",
    "I",
    "m",
    "p",
    "l",
    "e",
    "m",
    "e",
    "n",
    "t",
    " ",
    "a",
    " ",
    "m",
    "u",
    "l",
    "t",
    "i",
    "-",
    "c",
    "l",
    "a",
    "s",
    "s",
    " ",
    "c",
    "l",
    "a",
    "s",
    "s",
    "i",
    "f",
    "i",
    "c",
    "a",
    "t",
    "i",
    "o",
    "n",
    " ",
    "p",
    "r",
    "o",
    "b",
    "l",
    "e",
    "m",
    " ",
    "w",
    "i",
    "t",
    "h",
    " ",
    "3",
    " ",
    "o",
    "r",
    " ",
    "m",
    "o",
    "r",
    "e",
    " ",
    "c",
    "l",
    "a",
    "s",
    "s",
    "e",
    "s",
    ".",
    " ",
    "H",
    "o",
    "w",
    " ",
    "d",
    "o",
    "e",
    "s",
    " ",
    "S",
    "V",
    "M",
    " ",
    "h",
    "a",
    "n",
    "d",
    "l",
    "e",
    " ",
    "m",
    "u",
    "l",
    "t",
    "i",
    "-",
    "c",
    "l",
    "a",
    "s",
    "s",
    " ",
    "p",
    "r",
    "o",
    "b",
    "l",
    "e",
    "m",
    "s",
    "?",
    "\n",
    "\n",
    "5",
    ".",
    " ",
    "*",
    "*",
    "E",
    "x",
    "e",
    "r",
    "c",
    "i",
    "s",
    "e",
    " ",
    "5",
    ":",
    "*",
    "*",
    " ",
    "C",
    "o",
    "m",
    "p",
    "a",
    "r",
    "e",
    " ",
    "S",
    "V",
    "M",
    " ",
    "w",
    "i",
    "t",
    "h",
    " ",
    "o",
    "t",
    "h",
    "e",
    "r",
    " ",
    "c",
    "l",
    "a",
    "s",
    "s",
    "i",
    "f",
    "i",
    "e",
    "r",
    "s",
    " ",
    "(",
    "N",
    "a",
    "i",
    "v",
    "e",
    " ",
    "B",
    "a",
    "y",
    "e",
    "s",
    ",",
    " ",
    "L",
    "o",
    "g",
    "i",
    "s",
    "t",
    "i",
    "c",
    " ",
    "R",
    "e",
    "g",
    "r",
    "e",
    "s",
    "s",
    "i",
    "o",
    "n",
    ",",
    " ",
    "R",
    "a",
    "n",
    "d",
    "o",
    "m",
    " ",
    "F",
    "o",
    "r",
    "e",
    "s",
    "t",
    ")",
    " ",
    "o",
    "n",
    " ",
    "t",
    "h",
    "e",
    " ",
    "s",
    "a",
    "m",
    "e",
    " ",
    "d",
    "a",
    "t",
    "a",
    "s",
    "e",
    "t",
    ".",
    " ",
    "W",
    "h",
    "i",
    "c",
    "h",
    " ",
    "p",
    "e",
    "r",
    "f",
    "o",
    "r",
    "m",
    "s",
    " ",
    "b",
    "e",
    "s",
    "t",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
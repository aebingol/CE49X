{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CE49X: Introduction to Computational Thinking and Data Science for Civil Engineers\n",
    "## Week 6 — Theory: Model Evaluation Philosophy\n",
    "\n",
    "**Instructor:** Dr. Eyuphan Koc  \n",
    "**Department of Civil Engineering, Bogazici University**  \n",
    "**Semester:** Spring 2026\n",
    "\n",
    "*Companion notebook to Week 6 lecture — designed for self-study and in-class discussion*\n",
    "\n",
    "---"
   ],
   "id": "60eaaac5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [The Accuracy Trap](#1.-The-Accuracy-Trap)\n",
    "2. [The Confusion Matrix](#2.-The-Confusion-Matrix)\n",
    "3. [Precision, Recall, and the Tradeoff](#3.-Precision,-Recall,-and-the-Tradeoff)\n",
    "4. [ROC Curves and AUC](#4.-ROC-Curves-and-AUC)\n",
    "5. [Overfitting — When Your Model Memorizes](#5.-Overfitting-—-When-Your-Model-Memorizes)\n",
    "6. [Data Leakage — The Silent Killer](#6.-Data-Leakage-—-The-Silent-Killer)\n",
    "7. [Putting It Together](#7.-Putting-It-Together)"
   ],
   "id": "e9666920"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. The Accuracy Trap\n",
    "\n",
    "Every beginner's first question about a model is: *\"What's the accuracy?\"*\n",
    "\n",
    "This section shows why that question, on its own, can be dangerously misleading — especially in civil engineering applications where failures are rare but catastrophic."
   ],
   "id": "dcc9a438"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, classification_report,\n",
    "                             precision_recall_curve, roc_curve, auc, \n",
    "                             mean_squared_error, r2_score)\n",
    "%matplotlib inline"
   ],
   "id": "3fa7214b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Structural health monitoring dataset — HIGHLY IMBALANCED\n",
    "n_safe = 990\n",
    "n_damaged = 10\n",
    "\n",
    "# Features: vibration_freq, max_deflection, crack_width_index\n",
    "X_safe = np.column_stack([\n",
    "    np.random.normal(5.0, 0.5, n_safe),    # vibration frequency (Hz)\n",
    "    np.random.normal(2.0, 0.3, n_safe),    # max deflection (mm)\n",
    "    np.random.normal(0.1, 0.05, n_safe)    # crack width index\n",
    "])\n",
    "X_damaged = np.column_stack([\n",
    "    np.random.normal(3.5, 0.8, n_damaged),\n",
    "    np.random.normal(4.0, 0.5, n_damaged),\n",
    "    np.random.normal(0.5, 0.1, n_damaged)\n",
    "])\n",
    "\n",
    "X = np.vstack([X_safe, X_damaged])\n",
    "y = np.array([0] * n_safe + [1] * n_damaged)  # 0=safe, 1=damaged\n",
    "\n",
    "print(f\"Dataset: {len(X)} bridges\")\n",
    "print(f\"  Safe:    {n_safe} ({n_safe/len(X)*100:.1f}%)\")\n",
    "print(f\"  Damaged: {n_damaged} ({n_damaged/len(X)*100:.1f}%)\")"
   ],
   "id": "b7aac4f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A model that ALWAYS predicts \"safe\"\n",
    "class AlwaysSafeModel:\n",
    "    def predict(self, X):\n",
    "        return np.zeros(len(X))\n",
    "\n",
    "dumb_model = AlwaysSafeModel()\n",
    "y_pred_dumb = dumb_model.predict(X)\n",
    "accuracy_dumb = accuracy_score(y, y_pred_dumb)\n",
    "print(f\"Accuracy of the 'always safe' model: {accuracy_dumb:.1%}\")\n",
    "print(f\"\\nBut how many damaged bridges did it catch?\")\n",
    "print(f\"Damaged bridges detected: {np.sum(y_pred_dumb[y == 1]):.0f} out of {n_damaged}\")"
   ],
   "id": "2f2ac2e2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Key Insight: Accuracy is Meaningless When Classes are Imbalanced**\n",
    "> The model above has 99% accuracy — and it's **completely useless**. It missed ALL 10 damaged bridges.\n",
    ">\n",
    "> In engineering, failures are rare (thankfully). But that makes accuracy a terrible metric for safety-critical applications.\n",
    "\n",
    "[DISCUSS] Can you think of other engineering scenarios where the \"rare event\" is the one you care about most?"
   ],
   "id": "1697e337"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. The Confusion Matrix\n",
    "\n",
    "The confusion matrix tells us **how** the model is wrong — not just whether it's wrong. This distinction matters enormously when different types of errors have different consequences."
   ],
   "id": "ef8b7035"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "model = LogisticRegression(random_state=42, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "im = ax.imshow(cm, cmap='Blues')\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_xticklabels(['Predicted Safe', 'Predicted Damaged'], fontsize=11)\n",
    "ax.set_yticklabels(['Actually Safe', 'Actually Damaged'], fontsize=11)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        label = f'{cm[i, j]}'\n",
    "        if i == 0 and j == 0: label += '\\n(TN)'\n",
    "        elif i == 0 and j == 1: label += '\\n(FP)'\n",
    "        elif i == 1 and j == 0: label += '\\n(FN)'\n",
    "        else: label += '\\n(TP)'\n",
    "        ax.text(j, i, label, ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax)\n",
    "ax.set_title('Confusion Matrix — Bridge Safety Classifier', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "1ddd13f6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Definition: Confusion Matrix Quadrants**\n",
    ">\n",
    "> | | Predicted Negative | Predicted Positive |\n",
    "> |---|---|---|\n",
    "> | **Actually Negative** | True Negative (TN) | False Positive (FP) |\n",
    "> | **Actually Positive** | False Negative (FN) | True Positive (TP) |\n",
    ">\n",
    "> - **TN**: Correctly identified as safe\n",
    "> - **FP**: Safe bridge flagged as damaged (\"false alarm\")\n",
    "> - **FN**: Damaged bridge missed (\"missed detection\")\n",
    "> - **TP**: Correctly identified as damaged"
   ],
   "id": "ae472931"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Key Insight: In Civil Engineering, Errors are NOT Symmetric**\n",
    ">\n",
    "> | Error Type | Meaning | Consequence |\n",
    "> |---|---|---|\n",
    "> | **False Negative (FN)** | Damaged bridge predicted as safe | **People could die** |\n",
    "> | **False Positive (FP)** | Safe bridge predicted as damaged | Unnecessary inspection (costs money) |\n",
    ">\n",
    "> A false negative in structural health monitoring can be catastrophic. A false positive is merely expensive."
   ],
   "id": "6e1902da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two models, same accuracy (~95%), very different confusion matrices\n",
    "print(\"=== Model A ===\")\n",
    "print(\"Accuracy: 95.0%\")\n",
    "print(\"  TN=285  FP=12\")\n",
    "print(\"  FN=3    TP=0\")\n",
    "print(\"  → Caught 0 out of 3 damaged bridges\")\n",
    "print()\n",
    "print(\"=== Model B ===\")\n",
    "print(\"Accuracy: 95.0%\")\n",
    "print(\"  TN=282  FP=15\")\n",
    "print(\"  FN=0    TP=3\")\n",
    "print(\"  → Caught ALL 3 damaged bridges (with 15 false alarms)\")\n",
    "print()\n",
    "print(\"[DISCUSS] Which model would you deploy on a real bridge?\")"
   ],
   "id": "e07d6c0d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [QUICK] Full classification report from sklearn\n",
    "print(\"Classification Report — Bridge Safety Classifier\")\n",
    "print(\"=\" * 55)\n",
    "print(classification_report(y_test, y_pred, target_names=['Safe', 'Damaged']))"
   ],
   "id": "0f47adff"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Precision, Recall, and the Tradeoff\n",
    "\n",
    "Now that we understand the confusion matrix, we can define two critical metrics:\n",
    "\n",
    "> **Definition: Precision and Recall**\n",
    ">\n",
    "> **Precision** = TP / (TP + FP) — *\"Of all alarms raised, how many were real?\"*\n",
    ">\n",
    "> **Recall** = TP / (TP + FN) — *\"Of all actual positives, how many did we catch?\"*\n",
    ">\n",
    "> These two metrics are in tension. Improving one typically hurts the other."
   ],
   "id": "eb808dd5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "thresholds = np.arange(0.1, 0.95, 0.05)\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred_t = (y_proba >= t).astype(int)\n",
    "    tp = np.sum((y_pred_t == 1) & (y_test == 1))\n",
    "    fp = np.sum((y_pred_t == 1) & (y_test == 0))\n",
    "    fn = np.sum((y_pred_t == 0) & (y_test == 1))\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    rec = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    precisions.append(prec)\n",
    "    recalls.append(rec)\n",
    "\n",
    "# Show a few key thresholds\n",
    "print(f\"{'Threshold':>10} {'Precision':>10} {'Recall':>10} {'Interpretation':>40}\")\n",
    "print(\"-\" * 75)\n",
    "for i in range(0, len(thresholds), 3):\n",
    "    t = thresholds[i]\n",
    "    p = precisions[i]\n",
    "    r = recalls[i]\n",
    "    interp = \"high recall (catch more)\" if t < 0.3 else (\"balanced\" if t < 0.6 else \"high precision (fewer false alarms)\")\n",
    "    print(f\"{t:>10.2f} {p:>10.2f} {r:>10.2f} {interp:>40}\")"
   ],
   "id": "5ec619ac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_proba)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(recall_curve, precision_curve, 'b-', linewidth=2)\n",
    "ax.set_xlabel('Recall (How many damaged bridges we catch)', fontsize=12)\n",
    "ax.set_ylabel('Precision (How many alarms are real)', fontsize=12)\n",
    "ax.set_title('Precision-Recall Tradeoff', fontsize=13)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim([0, 1.05])\n",
    "ax.set_ylim([0, 1.05])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "ef44be59"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Key Insight: You Can't Maximize Both**\n",
    "> - **Optimize Recall** when missing a positive is catastrophic → structural safety monitoring\n",
    "> - **Optimize Precision** when false alarms are costly → routine maintenance scheduling\n",
    "> - **F1 Score** = harmonic mean of precision and recall — use when you want one number that balances both"
   ],
   "id": "c74c7a2f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Example: Civil Engineering Applications**\n",
    "> - **Structural safety** → High recall (don't miss damage, even if you get some false alarms)\n",
    "> - **Routine maintenance** → High precision (don't waste inspections on false alarms)\n",
    "> - **Emergency response** → High recall (better safe than sorry)"
   ],
   "id": "ad43e8b0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [PRACTICE] Compute F1 score manually\n",
    "# F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "tp = np.sum((y_pred == 1) & (y_test == 1))\n",
    "fp = np.sum((y_pred == 1) & (y_test == 0))\n",
    "fn = np.sum((y_pred == 0) & (y_test == 1))\n",
    "\n",
    "precision_val = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall_val = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1_val = 2 * (precision_val * recall_val) / (precision_val + recall_val) if (precision_val + recall_val) > 0 else 0\n",
    "\n",
    "print(f\"Precision: {precision_val:.3f}\")\n",
    "print(f\"Recall:    {recall_val:.3f}\")\n",
    "print(f\"F1 Score:  {f1_val:.3f}\")\n",
    "print(f\"\\n[TOGETHER] What does this F1 score tell us about the model's balance?\")"
   ],
   "id": "2e02b8a5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. ROC Curves and AUC\n",
    "\n",
    "The **Receiver Operating Characteristic (ROC)** curve gives us a way to evaluate a classifier across **all possible thresholds** simultaneously. The **Area Under the Curve (AUC)** summarizes this into a single number."
   ],
   "id": "4ce619ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Train multiple models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, class_weight='balanced'),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42, class_weight='balanced'),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Guessing (AUC=0.50)')\n",
    "\n",
    "for name, m in models.items():\n",
    "    m.fit(X_train, y_train)\n",
    "    if hasattr(m, 'predict_proba'):\n",
    "        y_score = m.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_score = m.decision_function(X_test)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax.plot(fpr, tpr, linewidth=2, label=f'{name} (AUC={roc_auc:.2f})')\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('ROC Curves — Comparing Models', fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "e7d0ab60"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Key Insight: AUC Tells You How Well Your Model Separates Classes**\n",
    "> - AUC = 1.0: Perfect separation\n",
    "> - AUC = 0.5: Random guessing (the diagonal)\n",
    "> - Higher AUC = better model, regardless of threshold choice"
   ],
   "id": "60f39048"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Definition: ROC Curve Axes**\n",
    ">\n",
    "> - **X-axis (False Positive Rate)** = FP / (FP + TN) — *\"Of all safe bridges, how many did we falsely flag?\"*\n",
    "> - **Y-axis (True Positive Rate = Recall)** = TP / (TP + FN) — *\"Of all damaged bridges, how many did we catch?\"*\n",
    ">\n",
    "> A perfect model hugs the **top-left corner**. A random model lies along the **diagonal**."
   ],
   "id": "7d3d87c9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [QUICK] Compare AUC values numerically\n",
    "print(\"Model Comparison Summary\")\n",
    "print(\"=\" * 50)\n",
    "for name, m in models.items():\n",
    "    y_pred_m = m.predict(X_test)\n",
    "    if hasattr(m, 'predict_proba'):\n",
    "        y_score_m = m.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_score_m = m.decision_function(X_test)\n",
    "    fpr_m, tpr_m, _ = roc_curve(y_test, y_score_m)\n",
    "    roc_auc_m = auc(fpr_m, tpr_m)\n",
    "    acc_m = accuracy_score(y_test, y_pred_m)\n",
    "    print(f\"{name:30s}  Accuracy={acc_m:.3f}  AUC={roc_auc_m:.3f}\")\n",
    "\n",
    "print(\"\\n[DISCUSS] Which model would you choose and why?\")\n",
    "print(\"Hint: AUC is often more informative than accuracy for imbalanced problems.\")"
   ],
   "id": "91661aa7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Overfitting — When Your Model Memorizes\n",
    "\n",
    "A model that performs perfectly on training data but poorly on new data has **memorized** the noise in the training set rather than learning the underlying pattern. This is one of the most fundamental challenges in machine learning."
   ],
   "id": "a23e0971"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_pts = 30\n",
    "x = np.sort(np.random.uniform(0, 10, n_pts))\n",
    "y_true = np.sin(x)\n",
    "y_noisy = y_true + np.random.normal(0, 0.3, n_pts)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 4))\n",
    "x_plot = np.linspace(0, 10, 300)\n",
    "\n",
    "for ax, degree in zip(axes, [1, 3, 10, 20]):\n",
    "    coeffs = np.polyfit(x, y_noisy, degree)\n",
    "    y_fit = np.polyval(coeffs, x_plot)\n",
    "    \n",
    "    ax.scatter(x, y_noisy, s=40, color='steelblue', edgecolors='k', zorder=3)\n",
    "    ax.plot(x_plot, y_fit, 'r-', linewidth=2)\n",
    "    ax.plot(x_plot, np.sin(x_plot), 'g--', linewidth=1, alpha=0.5, label='True function')\n",
    "    ax.set_title(f'Degree {degree}', fontsize=12)\n",
    "    ax.set_ylim(-2, 2)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    if degree == 1: ax.legend(fontsize=9)\n",
    "\n",
    "plt.suptitle('Polynomial Fits: From Underfitting to Overfitting', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "4e767cac"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Definition: Underfitting vs. Overfitting**\n",
    ">\n",
    "> - **Underfitting** (degree 1): Model is too simple to capture the pattern. High bias, low variance.\n",
    "> - **Just right** (degree 3): Model captures the pattern without fitting the noise.\n",
    "> - **Overfitting** (degree 20): Model memorizes the noise. Low bias, high variance.\n",
    ">\n",
    "> The goal is to find the **sweet spot** — complex enough to capture real patterns, simple enough to generalize."
   ],
   "id": "59e5499c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Split data\n",
    "x_train_ov, x_test_ov, y_train_ov, y_test_ov = train_test_split(\n",
    "    x, y_noisy, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "degrees = range(1, 16)\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for d in degrees:\n",
    "    pipe = make_pipeline(PolynomialFeatures(d), LinearRegression())\n",
    "    pipe.fit(x_train_ov.reshape(-1, 1), y_train_ov)\n",
    "    \n",
    "    train_pred = pipe.predict(x_train_ov.reshape(-1, 1))\n",
    "    test_pred = pipe.predict(x_test_ov.reshape(-1, 1))\n",
    "    \n",
    "    train_errors.append(np.sqrt(mean_squared_error(y_train_ov, train_pred)))\n",
    "    test_errors.append(np.sqrt(mean_squared_error(y_test_ov, test_pred)))\n",
    "\n",
    "# Cap very large test errors for visualization\n",
    "test_errors_capped = [min(e, 5) for e in test_errors]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(list(degrees), train_errors, 'o-', color='steelblue', linewidth=2, label='Training Error (RMSE)')\n",
    "ax.plot(list(degrees), test_errors_capped, 's-', color='indianred', linewidth=2, label='Test Error (RMSE)')\n",
    "ax.set_xlabel('Polynomial Degree', fontsize=12)\n",
    "ax.set_ylabel('RMSE', fontsize=12)\n",
    "ax.set_title('Train vs Test Error: The Overfitting Curve', fontsize=13)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axvline(x=3, color='green', linestyle='--', alpha=0.5, label='Sweet spot')\n",
    "ax.legend(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "d837f199"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Key Insight: A Model That's Perfect on Training Data is Suspicious, Not Impressive**\n",
    "> - Training error always decreases with complexity\n",
    "> - Test error decreases, then **increases** (the U-curve)\n",
    "> - The gap between train and test error = overfitting"
   ],
   "id": "e2c92eea"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Example: Civil Engineering**\n",
    "> If your concrete strength model fits your lab data perfectly but fails on site data, it memorized the lab conditions instead of learning the physics."
   ],
   "id": "bd6b25e4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TOGETHER] Let's look at the numbers\n",
    "print(f\"{'Degree':>8} {'Train RMSE':>12} {'Test RMSE':>12} {'Gap':>10} {'Status':>15}\")\n",
    "print(\"-\" * 60)\n",
    "for d, tr_e, te_e in zip(degrees, train_errors, test_errors):\n",
    "    gap = te_e - tr_e\n",
    "    if gap < 0.1:\n",
    "        status = \"underfitting\"\n",
    "    elif gap < 0.5:\n",
    "        status = \"good fit\"\n",
    "    elif gap < 2.0:\n",
    "        status = \"overfitting\"\n",
    "    else:\n",
    "        status = \"SEVERE overfit\"\n",
    "    te_display = min(te_e, 99.99)\n",
    "    print(f\"{d:>8d} {tr_e:>12.4f} {te_display:>12.4f} {gap:>10.4f} {status:>15}\")"
   ],
   "id": "ac834acd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Data Leakage — The Silent Killer\n",
    "\n",
    "Data leakage occurs when information from outside the training dataset is used to create the model. It leads to **overly optimistic performance estimates** that collapse in production.\n",
    "\n",
    "This is arguably the most common and most dangerous mistake in applied machine learning."
   ],
   "id": "b3363ae1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n = 200\n",
    "\n",
    "# Concrete mix data\n",
    "cement = np.random.uniform(280, 400, n)\n",
    "water_cement = np.random.uniform(0.35, 0.60, n)\n",
    "strength_7d = 10 + 0.05 * cement - 15 * water_cement + np.random.normal(0, 2, n)\n",
    "strength_28d = 1.3 * strength_7d + 5 + np.random.normal(0, 3, n)\n",
    "\n",
    "# Create a \"leaked\" feature — strength category derived FROM the 28-day target\n",
    "strength_category = np.where(strength_28d > 35, 2, np.where(strength_28d > 25, 1, 0))\n",
    "\n",
    "df_leak = pd.DataFrame({\n",
    "    'cement_kg_m3': cement,\n",
    "    'water_cement': water_cement,\n",
    "    'strength_7d': strength_7d,\n",
    "    'strength_category': strength_category,  # LEAKED!\n",
    "    'strength_28d': strength_28d  # TARGET\n",
    "})\n",
    "\n",
    "print(\"Features available:\")\n",
    "print(df_leak.columns.tolist())\n",
    "print(\"\\n'strength_category' was derived FROM strength_28d — this is data leakage!\")"
   ],
   "id": "04fec6d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITH leakage\n",
    "X_leaked = df_leak[['cement_kg_m3', 'water_cement', 'strength_7d', 'strength_category']].values\n",
    "y_target = df_leak['strength_28d'].values\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X_leaked, y_target, test_size=0.2, random_state=42)\n",
    "model_leaked = LinearRegression().fit(X_tr, y_tr)\n",
    "\n",
    "# WITHOUT leakage\n",
    "X_clean = df_leak[['cement_kg_m3', 'water_cement', 'strength_7d']].values\n",
    "X_tr2, X_te2, y_tr2, y_te2 = train_test_split(X_clean, y_target, test_size=0.2, random_state=42)\n",
    "model_clean = LinearRegression().fit(X_tr2, y_tr2)\n",
    "\n",
    "print(f\"WITH leaked feature:    R² = {model_leaked.score(X_te, y_te):.3f}  <- Too good to be true!\")\n",
    "print(f\"WITHOUT leaked feature: R² = {model_clean.score(X_te2, y_te2):.3f}  <- The real performance\")\n",
    "print(f\"\\nThe leaked model looks amazing but would fail in production,\")\n",
    "print(f\"because 'strength_category' wouldn't be available at prediction time.\")"
   ],
   "id": "607c9f2c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Key Insight: Data Leakage Gives You False Confidence**\n",
    "> Always ask: *\"Would I have this feature at prediction time?\"*\n",
    ">\n",
    "> **3 Common Leakage Patterns:**\n",
    "> 1. **Target leakage**: A feature derived from the target variable\n",
    "> 2. **Train-test contamination**: Scaling/normalizing before splitting\n",
    "> 3. **Temporal leakage**: Using future data to predict the past"
   ],
   "id": "fe28c8d2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [PRACTICE] Train-test contamination example\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# WRONG way: scale before split\n",
    "scaler_wrong = StandardScaler()\n",
    "X_scaled_wrong = scaler_wrong.fit_transform(X_clean)  # fit on ALL data\n",
    "Xw_tr, Xw_te, yw_tr, yw_te = train_test_split(X_scaled_wrong, y_target, test_size=0.2, random_state=42)\n",
    "model_wrong = LinearRegression().fit(Xw_tr, yw_tr)\n",
    "\n",
    "# RIGHT way: scale after split\n",
    "Xr_tr, Xr_te, yr_tr, yr_te = train_test_split(X_clean, y_target, test_size=0.2, random_state=42)\n",
    "scaler_right = StandardScaler()\n",
    "Xr_tr_scaled = scaler_right.fit_transform(Xr_tr)     # fit ONLY on training\n",
    "Xr_te_scaled = scaler_right.transform(Xr_te)          # transform test with train stats\n",
    "model_right = LinearRegression().fit(Xr_tr_scaled, yr_tr)\n",
    "\n",
    "print(\"Train-Test Contamination Demo\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"WRONG (scale before split): R² = {model_wrong.score(Xw_te, yw_te):.4f}\")\n",
    "print(f\"RIGHT (scale after split):  R² = {model_right.score(Xr_te_scaled, yr_te):.4f}\")\n",
    "print(f\"\\nIn this case the difference is small, but with more features\")\n",
    "print(f\"and less data, contamination can drastically inflate scores.\")"
   ],
   "id": "dd24c68a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Example: Civil Engineering**\n",
    ">\n",
    "> Imagine you are predicting 28-day concrete compressive strength at mix design time.\n",
    ">\n",
    "> - **Available at prediction time**: cement content, water-cement ratio, aggregate size, admixture type\n",
    "> - **NOT available at prediction time**: 7-day strength (you'd need to wait 7 days!), slump test results from the pour\n",
    ">\n",
    "> Including 7-day strength as a feature is only valid if you explicitly frame the problem as \"predict 28-day from 7-day results.\"\n",
    "> The key is **clarity about when the prediction needs to happen**."
   ],
   "id": "f2f1b2aa"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Putting It Together\n",
    "\n",
    "### Model Evaluation Checklist\n",
    "\n",
    "Before you report results, ask yourself:\n",
    "\n",
    "| # | Question | If \"No\", Then... |\n",
    "|---|---|---|\n",
    "| 1 | Is my dataset balanced? | Accuracy is misleading — use precision, recall, F1 |\n",
    "| 2 | Do I know the cost of each error type? | Define: What's worse, FP or FN? |\n",
    "| 3 | Am I evaluating on truly unseen data? | Use train-test split or cross-validation |\n",
    "| 4 | Does my model generalize? | Check train vs test error gap |\n",
    "| 5 | Could there be data leakage? | Audit every feature: would I have it at prediction time? |\n",
    "\n",
    "> **Key Insight: Model Evaluation is Not a Step — It's a Mindset**\n",
    "> Evaluation pervades the **entire** workflow. From data collection (is this representative?) to deployment (is the model still accurate on new data?)."
   ],
   "id": "4d98e3fa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TOGETHER] Summary quiz\n",
    "print(\"Quick Self-Check\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(\"Q1: Your earthquake damage classifier has 98% accuracy.\")\n",
    "print(\"    Should you celebrate? Why or why not?\")\n",
    "print()\n",
    "print(\"Q2: Your model has perfect R² on training data.\")\n",
    "print(\"    What should you suspect?\")\n",
    "print()\n",
    "print(\"Q3: You're predicting landslide risk. Which is worse:\")\n",
    "print(\"    predicting 'safe' when it's dangerous (FN), or\")\n",
    "print(\"    predicting 'dangerous' when it's safe (FP)?\")\n",
    "print()\n",
    "print(\"Q4: Your concrete strength model uses 'quality_grade'\")\n",
    "print(\"    as a feature. This grade was assigned AFTER the\")\n",
    "print(\"    28-day test. Is this a problem?\")"
   ],
   "id": "1a58d621"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection to the CE49X Course\n",
    "\n",
    "This notebook gives you the evaluation framework for:\n",
    "- **Week 07** (Naive Bayes): Before reporting \"my Naive Bayes has 94% accuracy,\" ask — *accuracy at what cost?*\n",
    "- **Week 08** (SVM): Before choosing an SVM kernel, check — *am I overfitting to the training set?*\n",
    "\n",
    "> **Key Insight: The Right Question**\n",
    "> Don't ask: \"What's my model's accuracy?\"\n",
    "> Ask: \"What's the cost of my model's errors, and can I live with it?\"\n",
    "\n",
    "---\n",
    "\n",
    "### Questions?\n",
    "\n",
    "**Dr. Eyuphan Koc**  \n",
    "eyuphan.koc@bogazici.edu.tr"
   ],
   "id": "467f2f46"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat": 4,
   "nbformat_minor": 5,
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
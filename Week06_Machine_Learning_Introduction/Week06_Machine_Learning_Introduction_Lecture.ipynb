{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CE49X: Introduction to Computational Thinking and Data Science for Civil Engineers\n",
    "## Week 6: Introduction to Machine Learning\n",
    "\n",
    "**Instructor:** Dr. Eyuphan Koc  \n",
    "**Department of Civil Engineering, Bogazici University**  \n",
    "**Semester:** Spring 2026\n",
    "\n",
    "---\n",
    "\n",
    "Based on *Python Data Science Handbook* by Jake VanderPlas  \n",
    "Chapter 5: Machine Learning (Sections 5.0--5.6)  \n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [What is Machine Learning?](#1.-What-is-Machine-Learning?)\n",
    "2. [Introducing Scikit-Learn](#2.-Introducing-Scikit-Learn)\n",
    "3. [Hyperparameters and Model Validation](#3.-Hyperparameters-and-Model-Validation)\n",
    "4. [Linear Regression](#4.-Linear-Regression)\n",
    "5. [Summary and Next Steps](#5.-Summary-and-Next-Steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. What is Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Machine Learning?\n",
    "\n",
    "**Definition:**\n",
    "- Machine Learning is about building **mathematical models** to understand data\n",
    "- Fundamentally, it's a **data-driven approach** to learning patterns\n",
    "- Models learn from examples rather than explicit programming\n",
    "\n",
    "**Key Idea:**\n",
    "\n",
    "> *\"Instead of programming explicit rules, we provide examples and let the algorithm discover the patterns.\"*\n",
    "\n",
    "> **Example: Civil Engineering**  \n",
    "> Rather than coding rules for predicting concrete strength, we provide examples of mix designs and their measured strengths -- the model learns the relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories of Machine Learning\n",
    "\n",
    "| | **Supervised Learning** | **Unsupervised Learning** |\n",
    "|---|---|---|\n",
    "| **Data** | Learn from **labeled** data | Learn from **unlabeled** data |\n",
    "| **Setup** | Have input-output pairs | No predefined outputs |\n",
    "| **Goal** | Predict outputs for new inputs | Discover structure |\n",
    "| **Type 1** | **Classification**: Predict discrete labels | **Clustering**: Group similar items |\n",
    "| **Type 2** | **Regression**: Predict continuous values | **Dimensionality Reduction**: Compress data |\n",
    "\n",
    "> **Key Insight: Key Difference**  \n",
    "> Supervised: \"Here are examples with answers\" vs Unsupervised: \"Find patterns in this data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning: Classification vs Regression\n",
    "\n",
    "| | **Classification** | **Regression** |\n",
    "|---|---|---|\n",
    "| **Predict** | **Discrete categories** | **Continuous values** |\n",
    "| **Output** | A label or class | A number |\n",
    "| **General Examples** | Email: spam or not spam? | House price prediction |\n",
    "| | Image: cat or dog? | Temperature forecasting |\n",
    "| **CE Examples** | Soil type: clay, sand, or silt? | Concrete strength from mix design |\n",
    "| | Structure: safe or unsafe? | Bridge deflection under load |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Classification and Regression\n",
    "\n",
    "The following plot shows the key difference between classification (left) and regression (right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Classification plot\n",
    "ax = axes[0]\n",
    "# Class A (blue)\n",
    "ax.scatter([0.5, 0.8, 0.6], [0.5, 0.7, 1.0], c='steelblue', s=80, label='Class A', edgecolors='k', zorder=3)\n",
    "# Class B (red)\n",
    "ax.scatter([2.0, 2.3, 1.8], [1.5, 1.8, 2.0], c='indianred', s=80, label='Class B', edgecolors='k', zorder=3)\n",
    "ax.set_xlabel('$x_1$', fontsize=12)\n",
    "ax.set_ylabel('$x_2$', fontsize=12)\n",
    "ax.set_title('Classification', fontsize=14)\n",
    "ax.legend(fontsize=10)\n",
    "ax.set_xlim(0, 3)\n",
    "ax.set_ylim(0, 2.5)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Regression plot\n",
    "ax = axes[1]\n",
    "x_pts = np.array([0.5, 1.0, 1.5, 2.0, 2.5])\n",
    "y_pts = np.array([0.6, 1.0, 1.3, 1.7, 2.0])\n",
    "ax.scatter(x_pts, y_pts, c='indianred', s=80, edgecolors='k', zorder=3, label='Data points')\n",
    "# Regression line\n",
    "x_line = np.linspace(0.3, 2.7, 100)\n",
    "y_line = 0.3 + 0.7 * x_line\n",
    "ax.plot(x_line, y_line, 'steelblue', linewidth=2, label='Fit line')\n",
    "ax.set_xlabel('$x$', fontsize=12)\n",
    "ax.set_ylabel('$y$', fontsize=12)\n",
    "ax.set_title('Regression', fontsize=14)\n",
    "ax.legend(fontsize=10)\n",
    "ax.set_xlim(0, 3)\n",
    "ax.set_ylim(0, 2.5)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning: Clustering vs Dimensionality Reduction\n",
    "\n",
    "| | **Clustering** | **Dimensionality Reduction** |\n",
    "|---|---|---|\n",
    "| **Goal** | Group similar data points | Reduce number of features |\n",
    "| **Labels** | No predefined labels | Preserve important information |\n",
    "| **Use** | Discover natural groupings | Visualization & compression |\n",
    "| **General Examples** | Customer segmentation | Image compression |\n",
    "| | Document organization | Feature extraction |\n",
    "| **CE Examples** | Grouping similar bridge designs | Compress sensor data streams |\n",
    "| | Identifying failure patterns | Visualize high-D material properties |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Clustering and Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Clustering plot\n",
    "ax = axes[0]\n",
    "cluster1_x = [0.5, 0.7, 0.4]\n",
    "cluster1_y = [0.5, 0.8, 0.9]\n",
    "cluster2_x = [2.0, 2.2, 1.9]\n",
    "cluster2_y = [1.5, 1.7, 1.9]\n",
    "ax.scatter(cluster1_x, cluster1_y, c='steelblue', s=80, edgecolors='k', zorder=3, label='Cluster 1')\n",
    "ax.scatter(cluster2_x, cluster2_y, c='indianred', s=80, edgecolors='k', zorder=3, label='Cluster 2')\n",
    "# Dashed circles around clusters\n",
    "circle1 = plt.Circle((0.55, 0.73), 0.35, fill=False, linestyle='--', color='steelblue', linewidth=2)\n",
    "circle2 = plt.Circle((2.03, 1.7), 0.35, fill=False, linestyle='--', color='indianred', linewidth=2)\n",
    "ax.add_patch(circle1)\n",
    "ax.add_patch(circle2)\n",
    "ax.set_xlabel('$x_1$', fontsize=12)\n",
    "ax.set_ylabel('$x_2$', fontsize=12)\n",
    "ax.set_title('Clustering', fontsize=14)\n",
    "ax.legend(fontsize=10)\n",
    "ax.set_xlim(0, 3)\n",
    "ax.set_ylim(0, 2.5)\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Dimensionality Reduction plot\n",
    "ax = axes[1]\n",
    "pts_x = [0.8, 1.2, 1.6, 2.0]\n",
    "pts_y = [0.7, 1.0, 1.3, 1.6]\n",
    "ax.scatter(pts_x, pts_y, c='indianred', s=80, edgecolors='k', zorder=3, label='Data points')\n",
    "ax.annotate('', xy=(2.5, 2.0), xytext=(0.3, 0.3),\n",
    "            arrowprops=dict(arrowstyle='->', color='steelblue', lw=2.5))\n",
    "ax.text(2.6, 2.05, 'PC1', fontsize=12, color='steelblue', fontweight='bold')\n",
    "ax.set_xlabel('$x_1$', fontsize=12)\n",
    "ax.set_ylabel('$x_2$', fontsize=12)\n",
    "ax.set_title('Dimensionality Reduction', fontsize=14)\n",
    "ax.legend(fontsize=10)\n",
    "ax.set_xlim(0, 3)\n",
    "ax.set_ylim(0, 2.5)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Workflow\n",
    "\n",
    "**Key Steps:**\n",
    "1. **Data Collection & Preprocessing:** Gather, clean, normalize data\n",
    "2. **Feature Engineering:** Select/create informative features\n",
    "3. **Model Selection & Training:** Choose algorithm, fit to data\n",
    "4. **Validation:** Test on unseen data, tune hyperparameters\n",
    "5. **Deployment:** Use model in production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Workflow Diagram\n",
    "\n",
    "```\n",
    " [Raw Data] --> [Preprocess & Clean] --> [Feature Engineering]\n",
    "                                                  |\n",
    "                                                  v\n",
    "                    [Validate] <-- [Train Model] <-- [Choose Model]\n",
    "                        |                ^                       \n",
    "                        |                |  (adjust)             \n",
    "                        +----------------+                       \n",
    "                        |\n",
    "                        v\n",
    "                     [Deploy]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Introducing Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Scikit-Learn?\n",
    "\n",
    "**Scikit-Learn** is Python's premier machine learning library.\n",
    "\n",
    "**Key Features:**\n",
    "- **Consistent API:** All models follow the same interface\n",
    "- **Comprehensive:** Classification, regression, clustering, dimensionality reduction\n",
    "- **Well-documented:** Excellent documentation and examples\n",
    "- **Built on NumPy/SciPy:** Fast and efficient\n",
    "- **Open-source:** Free and actively maintained\n",
    "\n",
    "**Installation:**\n",
    "```bash\n",
    "pip install scikit-learn\n",
    "# or\n",
    "conda install scikit-learn\n",
    "```\n",
    "\n",
    "> **Example: Why Scikit-Learn?**  \n",
    "> Unified interface means learning one model teaches you all models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Representation in Scikit-Learn\n",
    "\n",
    "**Two fundamental data structures:**\n",
    "\n",
    "| | **Features Matrix: $\\mathbf{X}$** | **Target Array: $\\mathbf{y}$** |\n",
    "|---|---|---|\n",
    "| **Shape** | $[n_{\\text{samples}}, n_{\\text{features}}]$ | $[n_{\\text{samples}}]$ |\n",
    "| **Content** | Each row = one sample, each column = one feature | Labels (classification) or values (regression) |\n",
    "| **Type** | 2D NumPy array or pandas DataFrame | 1D NumPy array or pandas Series |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Estimator API\n",
    "\n",
    "**All Scikit-Learn models follow the same pattern:**\n",
    "\n",
    "1. **Choose a model class** and import it\n",
    "2. **Choose hyperparameters** by instantiating the class\n",
    "3. **Arrange data** into features matrix $\\mathbf{X}$ and target vector $\\mathbf{y}$\n",
    "4. **Fit the model** to your data with `.fit()`\n",
    "5. **Apply the model** with `.predict()` or `.transform()`\n",
    "\n",
    "**Universal Interface:**\n",
    "```python\n",
    "from sklearn.some_module import SomeModel\n",
    "\n",
    "# 1. Choose model and hyperparameters\n",
    "model = SomeModel(hyperparameter1=value1,\n",
    "                  hyperparameter2=value2)\n",
    "\n",
    "# 2. Fit to data\n",
    "model.fit(X, y)\n",
    "\n",
    "# 3. Predict on new data\n",
    "predictions = model.predict(X_new)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#",
    "#",
    "#",
    " ",
    "[",
    "T",
    "O",
    "G",
    "E",
    "T",
    "H",
    "E",
    "R",
    "]",
    " ",
    "E",
    "x",
    "a",
    "m",
    "p",
    "l",
    "e",
    ":",
    " ",
    "S",
    "i",
    "m",
    "p",
    "l",
    "e",
    " ",
    "L",
    "i",
    "n",
    "e",
    "a",
    "r",
    " ",
    "R",
    "e",
    "g",
    "r",
    "e",
    "s",
    "s",
    "i",
    "o",
    "n",
    " ",
    "(",
    "W",
    "a",
    "t",
    "e",
    "r",
    "-",
    "C",
    "e",
    "m",
    "e",
    "n",
    "t",
    " ",
    "R",
    "a",
    "t",
    "i",
    "o",
    " ",
    "v",
    "s",
    " ",
    "C",
    "o",
    "n",
    "c",
    "r",
    "e",
    "t",
    "e",
    " ",
    "S",
    "t",
    "r",
    "e",
    "n",
    "g",
    "t",
    "h",
    ")",
    "\n",
    "\n",
    "*",
    "*",
    "P",
    "r",
    "o",
    "b",
    "l",
    "e",
    "m",
    ":",
    "*",
    "*",
    " ",
    "P",
    "r",
    "e",
    "d",
    "i",
    "c",
    "t",
    " ",
    "c",
    "o",
    "n",
    "c",
    "r",
    "e",
    "t",
    "e",
    " ",
    "s",
    "t",
    "r",
    "e",
    "n",
    "g",
    "t",
    "h",
    " ",
    "f",
    "r",
    "o",
    "m",
    " ",
    "w",
    "a",
    "t",
    "e",
    "r",
    "-",
    "c",
    "e",
    "m",
    "e",
    "n",
    "t",
    " ",
    "r",
    "a",
    "t",
    "i",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Generate sample data (water-cement ratio vs strength)\n",
    "X = np.array([[0.4], [0.45], [0.5], [0.55], [0.6], [0.65]])\n",
    "y = np.array([45, 40, 35, 30, 25, 20])  # Strength in MPa\n",
    "\n",
    "# 1. Choose model\n",
    "model = LinearRegression()\n",
    "\n",
    "# 2. Fit model\n",
    "model.fit(X, y)\n",
    "\n",
    "# 3. Make predictions\n",
    "X_new = np.array([[0.48], [0.58]])\n",
    "predictions = model.predict(X_new)\n",
    "\n",
    "print(f\"Predictions: {predictions}\")\n",
    "print(f\"Slope: {model.coef_[0]:.2f}\")\n",
    "print(f\"Intercept: {model.intercept_:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Regression Line\n",
    "\n",
    "Let's plot the original data points and the fitted regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a range of values for plotting the regression line\n",
    "X_line = np.linspace(0.30, 0.70, 100).reshape(-1, 1)\n",
    "y_line = model.predict(X_line)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the original data points\n",
    "plt.scatter(X, y, color='blue', s=100, alpha=0.6, label='Training Data')\n",
    "\n",
    "# Plot the regression line\n",
    "plt.plot(X_line, y_line, color='red', linewidth=2, label='Regression Line')\n",
    "\n",
    "# Plot the predictions\n",
    "plt.scatter(X_new, predictions, color='green', s=100, marker='s',\n",
    "            label='Predictions', zorder=5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Water-Cement Ratio', fontsize=12)\n",
    "plt.ylabel('Concrete Strength (MPa)', fontsize=12)\n",
    "plt.title('Linear Regression: Water-Cement Ratio vs Concrete Strength', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results Interpretation:**\n",
    "- **Slope:** The coefficient showing how much strength changes per unit change in water-cement ratio\n",
    "- **Intercept:** The y-intercept of the linear model\n",
    "- Higher water-cement ratio leads to lower concrete strength (negative slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#",
    "#",
    "#",
    " ",
    "[",
    "Q",
    "U",
    "I",
    "C",
    "K",
    "]",
    " ",
    "E",
    "x",
    "a",
    "m",
    "p",
    "l",
    "e",
    ":",
    " ",
    "C",
    "l",
    "a",
    "s",
    "s",
    "i",
    "f",
    "i",
    "c",
    "a",
    "t",
    "i",
    "o",
    "n",
    " ",
    "w",
    "i",
    "t",
    "h",
    " ",
    "I",
    "r",
    "i",
    "s",
    " ",
    "D",
    "a",
    "t",
    "a",
    "s",
    "e",
    "t",
    "\n",
    "\n",
    "*",
    "*",
    "P",
    "r",
    "o",
    "b",
    "l",
    "e",
    "m",
    ":",
    "*",
    "*",
    " ",
    "C",
    "l",
    "a",
    "s",
    "s",
    "i",
    "f",
    "y",
    " ",
    "i",
    "r",
    "i",
    "s",
    " ",
    "f",
    "l",
    "o",
    "w",
    "e",
    "r",
    "s",
    " ",
    "b",
    "a",
    "s",
    "e",
    "d",
    " ",
    "o",
    "n",
    " ",
    "p",
    "e",
    "t",
    "a",
    "l",
    "/",
    "s",
    "e",
    "p",
    "a",
    "l",
    " ",
    "m",
    "e",
    "a",
    "s",
    "u",
    "r",
    "e",
    "m",
    "e",
    "n",
    "t",
    "s",
    "\n",
    "\n",
    ">",
    " ",
    "*",
    "*",
    "E",
    "x",
    "a",
    "m",
    "p",
    "l",
    "e",
    ":",
    " ",
    "C",
    "i",
    "v",
    "i",
    "l",
    " ",
    "E",
    "n",
    "g",
    "i",
    "n",
    "e",
    "e",
    "r",
    "i",
    "n",
    "g",
    " ",
    "A",
    "n",
    "a",
    "l",
    "o",
    "g",
    "y",
    "*",
    "*",
    " ",
    " ",
    "\n",
    ">",
    " ",
    "R",
    "e",
    "p",
    "l",
    "a",
    "c",
    "e",
    " ",
    "i",
    "r",
    "i",
    "s",
    " ",
    "m",
    "e",
    "a",
    "s",
    "u",
    "r",
    "e",
    "m",
    "e",
    "n",
    "t",
    "s",
    " ",
    "w",
    "i",
    "t",
    "h",
    " ",
    "s",
    "o",
    "i",
    "l",
    " ",
    "p",
    "r",
    "o",
    "p",
    "e",
    "r",
    "t",
    "i",
    "e",
    "s",
    " ",
    "(",
    "g",
    "r",
    "a",
    "i",
    "n",
    " ",
    "s",
    "i",
    "z",
    "e",
    ",",
    " ",
    "m",
    "o",
    "i",
    "s",
    "t",
    "u",
    "r",
    "e",
    ",",
    " ",
    "d",
    "e",
    "n",
    "s",
    "i",
    "t",
    "y",
    ")",
    " ",
    "t",
    "o",
    " ",
    "c",
    "l",
    "a",
    "s",
    "s",
    "i",
    "f",
    "y",
    " ",
    "s",
    "o",
    "i",
    "l",
    " ",
    "t",
    "y",
    "p",
    "e",
    "s",
    " ",
    "(",
    "c",
    "l",
    "a",
    "y",
    ",",
    " ",
    "s",
    "i",
    "l",
    "t",
    ",",
    " ",
    "s",
    "a",
    "n",
    "d",
    ")",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "X_iris, y_iris = iris.data, iris.target\n",
    "\n",
    "# Split data: 80% training, 20% testing\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train model (k=3 nearest neighbors)\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_model.fit(X_train_iris, y_train_iris)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = knn_model.score(X_test_iris, y_test_iris)\n",
    "print(f\"Test Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning: PCA Example\n",
    "\n",
    "**Principal Component Analysis (PCA):** Reduce dimensionality while preserving variance\n",
    "\n",
    "> **Example: Engineering Application**  \n",
    "> Compress multi-sensor structural health monitoring data from 100 sensors to 5 principal components, retaining 95% of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load high-dimensional data (4 features)\n",
    "iris = load_iris()\n",
    "X_iris_full = iris.data  # Shape: (150, 4)\n",
    "\n",
    "# Reduce to 2 dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(X_iris_full)  # Shape: (150, 2)\n",
    "\n",
    "# How much variance is explained?\n",
    "print(f\"Explained variance: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total: {sum(pca.explained_variance_ratio_):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning: K-Means Clustering\n",
    "\n",
    "**K-Means:** Group data into $k$ clusters\n",
    "\n",
    "> **Example: Civil Engineering Application**  \n",
    "> Cluster bridge inspection data to identify structures with similar damage patterns for targeted maintenance strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Sample data: structural damage measurements\n",
    "X_damage = np.array([[1, 2], [1.5, 1.8], [5, 8],\n",
    "                     [8, 8], [1, 0.6], [9, 11]])\n",
    "\n",
    "# Create 2 clusters\n",
    "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "kmeans.fit(X_damage)\n",
    "\n",
    "# Get cluster labels\n",
    "labels = kmeans.labels_\n",
    "print(f\"Cluster assignments: {labels}\")\n",
    "\n",
    "# Get cluster centers\n",
    "centers = kmeans.cluster_centers_\n",
    "print(f\"Cluster centers:\\n{centers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Hyperparameters and Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Model Validation?\n",
    "\n",
    "**The Fundamental Problem:**\n",
    "- We want models that **generalize** to new, unseen data\n",
    "- Simply fitting training data is not enough\n",
    "- Need to estimate performance on future data\n",
    "\n",
    "> **Key Insight: Common Mistake -- Training on Test Data**  \n",
    "> **WRONG:** Evaluate model on the same data used for training  \n",
    "> **Result:** Overly optimistic performance estimates\n",
    "\n",
    "**Solution:** Hold out a separate **test set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 2))\n",
    "\n",
    "# Training data rectangle\n",
    "ax.barh(0, 80, left=0, height=0.5, color='steelblue', alpha=0.4, edgecolor='k')\n",
    "ax.text(40, 0, 'Training Data (80%)', ha='center', va='center', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Test data rectangle\n",
    "ax.barh(0, 20, left=80, height=0.5, color='indianred', alpha=0.4, edgecolor='k')\n",
    "ax.text(90, 0, 'Test (20%)', ha='center', va='center', fontsize=13, fontweight='bold')\n",
    "\n",
    "ax.set_xlim(-2, 102)\n",
    "ax.set_ylim(-0.6, 0.6)\n",
    "ax.set_xlabel('All Available Data', fontsize=12)\n",
    "ax.set_yticks([])\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "\n",
    "**Basic Approach:** Split data into training and testing sets\n",
    "\n",
    "> **Key Points**  \n",
    "> - `random_state`: ensures reproducibility  \n",
    "> - **Never** use test data during training or hyperparameter tuning  \n",
    "> - Test set estimates performance on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Use the iris data for a regression-style demo\n",
    "# Predict petal width from other measurements\n",
    "iris = load_iris()\n",
    "X_demo = iris.data[:, :3]  # sepal length, sepal width, petal length\n",
    "y_demo = iris.data[:, 3]   # petal width\n",
    "\n",
    "# Split: 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_demo, y_demo, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit on training data only\n",
    "model_demo = LinearRegression()\n",
    "model_demo.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test data\n",
    "train_score = model_demo.score(X_train, y_train)\n",
    "test_score = model_demo.score(X_test, y_test)\n",
    "\n",
    "print(f\"Training R^2: {train_score:.3f}\")\n",
    "print(f\"Test R^2: {test_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#",
    "#",
    "#",
    " ",
    "[",
    "T",
    "O",
    "G",
    "E",
    "T",
    "H",
    "E",
    "R",
    "]",
    " ",
    "C",
    "r",
    "o",
    "s",
    "s",
    "-",
    "V",
    "a",
    "l",
    "i",
    "d",
    "a",
    "t",
    "i",
    "o",
    "n",
    "\n",
    "\n",
    "*",
    "*",
    "P",
    "r",
    "o",
    "b",
    "l",
    "e",
    "m",
    " ",
    "w",
    "i",
    "t",
    "h",
    " ",
    "s",
    "i",
    "n",
    "g",
    "l",
    "e",
    " ",
    "t",
    "r",
    "a",
    "i",
    "n",
    "-",
    "t",
    "e",
    "s",
    "t",
    " ",
    "s",
    "p",
    "l",
    "i",
    "t",
    ":",
    "*",
    "*",
    "\n",
    "-",
    " ",
    "P",
    "e",
    "r",
    "f",
    "o",
    "r",
    "m",
    "a",
    "n",
    "c",
    "e",
    " ",
    "d",
    "e",
    "p",
    "e",
    "n",
    "d",
    "s",
    " ",
    "o",
    "n",
    " ",
    "w",
    "h",
    "i",
    "c",
    "h",
    " ",
    "s",
    "a",
    "m",
    "p",
    "l",
    "e",
    "s",
    " ",
    "e",
    "n",
    "d",
    "e",
    "d",
    " ",
    "u",
    "p",
    " ",
    "i",
    "n",
    " ",
    "t",
    "e",
    "s",
    "t",
    " ",
    "s",
    "e",
    "t",
    "\n",
    "-",
    " ",
    "W",
    "a",
    "s",
    "t",
    "e",
    "s",
    " ",
    "d",
    "a",
    "t",
    "a",
    " ",
    "(",
    "o",
    "n",
    "l",
    "y",
    " ",
    "8",
    "0",
    "%",
    " ",
    "u",
    "s",
    "e",
    "d",
    " ",
    "f",
    "o",
    "r",
    " ",
    "t",
    "r",
    "a",
    "i",
    "n",
    "i",
    "n",
    "g",
    ")",
    "\n",
    "\n",
    "*",
    "*",
    "S",
    "o",
    "l",
    "u",
    "t",
    "i",
    "o",
    "n",
    ":",
    " ",
    "K",
    "-",
    "F",
    "o",
    "l",
    "d",
    " ",
    "C",
    "r",
    "o",
    "s",
    "s",
    "-",
    "V",
    "a",
    "l",
    "i",
    "d",
    "a",
    "t",
    "i",
    "o",
    "n",
    "*",
    "*",
    "\n",
    "\n",
    "*",
    "*",
    "P",
    "r",
    "o",
    "c",
    "e",
    "s",
    "s",
    ":",
    "*",
    "*",
    "\n",
    "1",
    ".",
    " ",
    "S",
    "p",
    "l",
    "i",
    "t",
    " ",
    "d",
    "a",
    "t",
    "a",
    " ",
    "i",
    "n",
    "t",
    "o",
    " ",
    "$",
    "k",
    "$",
    " ",
    "e",
    "q",
    "u",
    "a",
    "l",
    " ",
    "p",
    "a",
    "r",
    "t",
    "s",
    " ",
    "(",
    "f",
    "o",
    "l",
    "d",
    "s",
    ")",
    "\n",
    "2",
    ".",
    " ",
    "T",
    "r",
    "a",
    "i",
    "n",
    " ",
    "o",
    "n",
    " ",
    "$",
    "k",
    "-",
    "1",
    "$",
    " ",
    "f",
    "o",
    "l",
    "d",
    "s",
    ",",
    " ",
    "t",
    "e",
    "s",
    "t",
    " ",
    "o",
    "n",
    " ",
    "t",
    "h",
    "e",
    " ",
    "r",
    "e",
    "m",
    "a",
    "i",
    "n",
    "i",
    "n",
    "g",
    " ",
    "f",
    "o",
    "l",
    "d",
    "\n",
    "3",
    ".",
    " ",
    "R",
    "e",
    "p",
    "e",
    "a",
    "t",
    " ",
    "$",
    "k",
    "$",
    " ",
    "t",
    "i",
    "m",
    "e",
    "s",
    ",",
    " ",
    "e",
    "a",
    "c",
    "h",
    " ",
    "f",
    "o",
    "l",
    "d",
    " ",
    "u",
    "s",
    "e",
    "d",
    " ",
    "a",
    "s",
    " ",
    "t",
    "e",
    "s",
    "t",
    " ",
    "s",
    "e",
    "t",
    " ",
    "o",
    "n",
    "c",
    "e",
    "\n",
    "4",
    ".",
    " ",
    "A",
    "v",
    "e",
    "r",
    "a",
    "g",
    "e",
    " ",
    "t",
    "h",
    "e",
    " ",
    "$",
    "k",
    "$",
    " ",
    "p",
    "e",
    "r",
    "f",
    "o",
    "r",
    "m",
    "a",
    "n",
    "c",
    "e",
    " ",
    "s",
    "c",
    "o",
    "r",
    "e",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross-Validation Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "n_folds = 5\n",
    "colors_train = 'steelblue'\n",
    "colors_test = 'indianred'\n",
    "\n",
    "for i in range(n_folds):\n",
    "    y_pos = n_folds - 1 - i\n",
    "    for j in range(n_folds):\n",
    "        x_start = j * (1.0 / n_folds)\n",
    "        width = 1.0 / n_folds\n",
    "        if j == i:\n",
    "            color = colors_test\n",
    "            alpha = 0.5\n",
    "            label = 'Test'\n",
    "        else:\n",
    "            color = colors_train\n",
    "            alpha = 0.4\n",
    "            label = 'Train'\n",
    "        rect = plt.Rectangle((x_start, y_pos - 0.35), width, 0.7,\n",
    "                             facecolor=color, alpha=alpha, edgecolor='k', linewidth=1)\n",
    "        ax.add_patch(rect)\n",
    "        if j == i:\n",
    "            ax.text(x_start + width / 2, y_pos, 'Test', ha='center', va='center',\n",
    "                    fontsize=10, fontweight='bold')\n",
    "    ax.text(-0.08, y_pos, f'Fold {i+1}:', ha='right', va='center', fontsize=11)\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=colors_train, alpha=0.4, edgecolor='k', label='Train'),\n",
    "                   Patch(facecolor=colors_test, alpha=0.5, edgecolor='k', label='Test')]\n",
    "ax.legend(handles=legend_elements, loc='upper right', fontsize=11)\n",
    "\n",
    "ax.set_xlim(-0.15, 1.05)\n",
    "ax.set_ylim(-0.6, n_folds - 0.2)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_title('5-Fold Cross-Validation', fontsize=14)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross-Validation in Scikit-Learn\n",
    "\n",
    "**Advantages:**\n",
    "- More robust performance estimate\n",
    "- Uses all data for both training and validation\n",
    "- Provides variance estimate (standard deviation)\n",
    "\n",
    "> **Example: Typical Choice**  \n",
    "> 5-fold or 10-fold cross-validation is standard. Use more folds for small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create model\n",
    "model_cv = LinearRegression()\n",
    "\n",
    "# Perform 5-fold cross-validation on the iris regression task\n",
    "scores = cross_val_score(model_cv, X_demo, y_demo, cv=5, scoring='r2')\n",
    "\n",
    "print(f\"Cross-validation scores: {scores}\")\n",
    "print(f\"Mean R^2: {scores.mean():.3f}\")\n",
    "print(f\"Std Dev: {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-Variance Tradeoff\n",
    "\n",
    "**Two sources of model error:**\n",
    "\n",
    "| | **Bias (Underfitting)** | **Variance (Overfitting)** |\n",
    "|---|---|---|\n",
    "| **Model** | Too simple | Too complex |\n",
    "| **Issue** | Cannot capture true pattern | Fits noise in training data |\n",
    "| **Training error** | High | Low |\n",
    "| **Test error** | High | High |\n",
    "| **Example** | Linear model for nonlinear data | High-degree polynomial |\n",
    "\n",
    "**Goal:** Find the sweet spot with minimum test error!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-Variance Tradeoff Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.linspace(0.5, 10, 200)\n",
    "\n",
    "# Training error: decreases with complexity\n",
    "train_err = 2.5 + 8 * np.exp(-x / 2)\n",
    "# Test error: decreases then increases (U-shape)\n",
    "test_err = 2.5 + 8 * np.exp(-x / 2) + 0.5 * x**2 / 8\n",
    "\n",
    "ax.plot(x, train_err, color='steelblue', linewidth=2.5, label='Training Error')\n",
    "ax.plot(x, test_err, color='indianred', linewidth=2.5, label='Test Error')\n",
    "\n",
    "# Mark regions\n",
    "ax.axvline(x=2.5, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.axvline(x=6.5, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.text(1.2, 1.0, 'Underfitting', fontsize=11, ha='center', style='italic')\n",
    "ax.text(4.5, 1.0, 'Good', fontsize=11, ha='center', style='italic', fontweight='bold')\n",
    "ax.text(8.2, 1.0, 'Overfitting', fontsize=11, ha='center', style='italic')\n",
    "\n",
    "ax.set_xlabel('Model Complexity', fontsize=13)\n",
    "ax.set_ylabel('Error', fontsize=13)\n",
    "ax.set_title('Bias-Variance Tradeoff', fontsize=14)\n",
    "ax.legend(fontsize=12)\n",
    "ax.set_xlim(0, 10.5)\n",
    "ax.set_ylim(0, 12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Curves\n",
    "\n",
    "**Validation Curve:** Plot performance vs. a single hyperparameter\n",
    "\n",
    "**Purpose:**\n",
    "- Visualize bias-variance tradeoff\n",
    "- Select optimal hyperparameter value\n",
    "- Diagnose under/overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Validation Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "degrees = np.arange(1, 16)\n",
    "\n",
    "# Simulated training score (always increases)\n",
    "train_score_sim = 0.3 + 0.6 * (1 - np.exp(-(degrees - 1) / 3))\n",
    "# Simulated validation score (increases then decreases)\n",
    "val_score_sim = 0.3 + 0.5 * (1 - np.exp(-(degrees - 1) / 3)) - 0.05 * (degrees - 5)**2 / 10\n",
    "\n",
    "ax.plot(degrees, train_score_sim, 'o-', color='steelblue', linewidth=2, markersize=6, label='Training Score')\n",
    "ax.plot(degrees, val_score_sim, 's-', color='indianred', linewidth=2, markersize=6, label='Validation Score')\n",
    "\n",
    "# Optimal line\n",
    "ax.axvline(x=5, color='green', linestyle='--', linewidth=2, alpha=0.7)\n",
    "ax.text(5.3, 0.9, 'Optimal', fontsize=12, color='green', fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Polynomial Degree', fontsize=13)\n",
    "ax.set_ylabel('Score ($R^2$)', fontsize=13)\n",
    "ax.set_title('Validation Curve', fontsize=14)\n",
    "ax.legend(fontsize=12)\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Validation Curves in Scikit-Learn\n",
    "\n",
    "> **Example: Engineering Application**  \n",
    "> Tune regularization strength when predicting structural response to prevent overfitting to measurement noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "\n",
    "# Test different regularization strengths\n",
    "param_range = np.logspace(-4, 4, 10)\n",
    "\n",
    "train_scores, val_scores = validation_curve(\n",
    "    Ridge(), X_demo, y_demo,\n",
    "    param_name='alpha',\n",
    "    param_range=param_range,\n",
    "    cv=5,\n",
    "    scoring='r2'\n",
    ")\n",
    "\n",
    "# Average across folds\n",
    "train_mean = train_scores.mean(axis=1)\n",
    "val_mean = val_scores.mean(axis=1)\n",
    "\n",
    "# Find best alpha\n",
    "best_alpha = param_range[val_mean.argmax()]\n",
    "print(f\"Best alpha: {best_alpha:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curves\n",
    "\n",
    "**Learning Curve:** Plot performance vs. training set size\n",
    "\n",
    "**Purpose:**\n",
    "- Diagnose whether more data will help\n",
    "- Identify high bias vs. high variance\n",
    "\n",
    "**Diagnosis:**\n",
    "- **Large gap:** High variance $\\rightarrow$ more data or regularization\n",
    "- **Converged low:** High bias $\\rightarrow$ more complex model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curves: High Variance vs High Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "train_sizes = np.linspace(10, 100, 50)\n",
    "\n",
    "# High Variance (Overfitting)\n",
    "ax = axes[0]\n",
    "train_score_hv = 0.95 - 0.3 * np.exp(-train_sizes / 30)\n",
    "val_score_hv = 0.75 * (1 - np.exp(-train_sizes / 30)) + 0.4\n",
    "ax.plot(train_sizes, train_score_hv, color='steelblue', linewidth=2.5, label='Training')\n",
    "ax.plot(train_sizes, val_score_hv, color='indianred', linewidth=2.5, label='Validation')\n",
    "ax.set_xlabel('Training Set Size', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('High Variance (Overfitting)', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_ylim(0.3, 1.05)\n",
    "ax.text(70, 0.5, 'Large gap\\nMore data helps', fontsize=10, ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# High Bias (Underfitting)\n",
    "ax = axes[1]\n",
    "train_score_hb = 0.55 + 0.05 * np.log10(train_sizes / 10 + 0.01)\n",
    "val_score_hb = 0.50 + 0.05 * np.log10(train_sizes / 10 + 0.01)\n",
    "ax.plot(train_sizes, train_score_hb, color='steelblue', linewidth=2.5, label='Training')\n",
    "ax.plot(train_sizes, val_score_hb, color='indianred', linewidth=2.5, label='Validation')\n",
    "ax.set_xlabel('Training Set Size', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('High Bias (Underfitting)', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_ylim(0.3, 1.05)\n",
    "ax.text(70, 0.8, 'Small gap\\nMore complexity\\nneeded', fontsize=10, ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#",
    "#",
    "#",
    " ",
    "[",
    "P",
    "R",
    "A",
    "C",
    "T",
    "I",
    "C",
    "E",
    "]",
    " ",
    "G",
    "r",
    "i",
    "d",
    " ",
    "S",
    "e",
    "a",
    "r",
    "c",
    "h",
    " ",
    "f",
    "o",
    "r",
    " ",
    "H",
    "y",
    "p",
    "e",
    "r",
    "p",
    "a",
    "r",
    "a",
    "m",
    "e",
    "t",
    "e",
    "r",
    " ",
    "T",
    "u",
    "n",
    "i",
    "n",
    "g",
    "\n",
    "\n",
    "*",
    "*",
    "P",
    "r",
    "o",
    "b",
    "l",
    "e",
    "m",
    ":",
    "*",
    "*",
    " ",
    "M",
    "a",
    "n",
    "y",
    " ",
    "m",
    "o",
    "d",
    "e",
    "l",
    "s",
    " ",
    "h",
    "a",
    "v",
    "e",
    " ",
    "m",
    "u",
    "l",
    "t",
    "i",
    "p",
    "l",
    "e",
    " ",
    "h",
    "y",
    "p",
    "e",
    "r",
    "p",
    "a",
    "r",
    "a",
    "m",
    "e",
    "t",
    "e",
    "r",
    "s",
    " ",
    "t",
    "o",
    " ",
    "t",
    "u",
    "n",
    "e",
    "\n",
    "\n",
    "*",
    "*",
    "G",
    "r",
    "i",
    "d",
    " ",
    "S",
    "e",
    "a",
    "r",
    "c",
    "h",
    ":",
    "*",
    "*",
    " ",
    "T",
    "r",
    "y",
    " ",
    "a",
    "l",
    "l",
    " ",
    "c",
    "o",
    "m",
    "b",
    "i",
    "n",
    "a",
    "t",
    "i",
    "o",
    "n",
    "s",
    " ",
    "o",
    "f",
    " ",
    "h",
    "y",
    "p",
    "e",
    "r",
    "p",
    "a",
    "r",
    "a",
    "m",
    "e",
    "t",
    "e",
    "r",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Use iris data for classification\n",
    "iris = load_iris()\n",
    "X_grid, y_grid = iris.data, iris.target\n",
    "X_train_grid, X_test_grid, y_train_grid, y_test_grid = train_test_split(\n",
    "    X_grid, y_grid, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1],\n",
    "    'kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "# Create grid search with 5-fold CV\n",
    "grid = GridSearchCV(SVC(), param_grid, cv=5,\n",
    "                    scoring='accuracy')\n",
    "\n",
    "# Fit searches all combinations\n",
    "grid.fit(X_train_grid, y_train_grid)\n",
    "\n",
    "print(f\"Best parameters: {grid.best_params_}\")\n",
    "print(f\"Best CV score: {grid.best_score_:.3f}\")\n",
    "\n",
    "# Use best model for predictions\n",
    "best_model = grid.best_estimator_\n",
    "print(f\"Test score: {best_model.score(X_test_grid, y_test_grid):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression: The Foundation\n",
    "\n",
    "**Goal:** Fit a linear relationship between features and target\n",
    "\n",
    "**Model:**\n",
    "\n",
    "$$\\hat{y} = w_0 + w_1 x_1 + w_2 x_2 + \\cdots + w_p x_p = w_0 + \\sum_{j=1}^{p} w_j x_j$$\n",
    "\n",
    "Where: $\\hat{y}$ = predicted value, $x_j$ = features, $w_j$ = weights, $w_0$ = intercept\n",
    "\n",
    "**Learning Objective:** Find weights that minimize error\n",
    "\n",
    "$$\\text{minimize} \\quad \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} \\left(y_i - w_0 - \\sum_{j=1}^{p} w_j x_{ij}\\right)^2$$\n",
    "\n",
    "> **Example: Civil Engineering**  \n",
    "> Predict concrete compressive strength from: cement content, water ratio, age, aggregate size, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear Regression Example: Age vs Concrete Strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Data: Age vs Concrete Strength\n",
    "age = np.array([3, 7, 14, 28, 56, 90])\n",
    "strength = np.array([20, 32, 38, 45, 50, 52])\n",
    "\n",
    "X_age = age.reshape(-1, 1)\n",
    "y_strength = strength\n",
    "\n",
    "# Fit model\n",
    "model_age = LinearRegression()\n",
    "model_age.fit(X_age, y_strength)\n",
    "\n",
    "# Coefficients\n",
    "print(f\"Slope: {model_age.coef_[0]:.2f} MPa/day\")\n",
    "print(f\"Intercept: {model_age.intercept_:.2f} MPa\")\n",
    "print(f\"R^2: {model_age.score(X_age, y_strength):.3f}\")\n",
    "\n",
    "# Predict\n",
    "age_new = np.array([[21], [42]])\n",
    "pred = model_age.predict(age_new)\n",
    "print(f\"\\nPredicted strength at 21 days: {pred[0]:.1f} MPa\")\n",
    "print(f\"Predicted strength at 42 days: {pred[1]:.1f} MPa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Fit Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Data points\n",
    "ax.scatter(age, strength, c='indianred', s=100, edgecolors='k', zorder=3, label='Measured Data')\n",
    "\n",
    "# Regression line\n",
    "x_plot = np.linspace(0, 100, 200).reshape(-1, 1)\n",
    "y_plot = model_age.predict(x_plot)\n",
    "ax.plot(x_plot, y_plot, color='steelblue', linewidth=2.5, label='Linear Fit')\n",
    "\n",
    "ax.set_xlabel('Age (days)', fontsize=13)\n",
    "ax.set_ylabel('Strength (MPa)', fontsize=13)\n",
    "ax.set_title('Concrete Age vs Compressive Strength', fontsize=14)\n",
    "ax.legend(fontsize=12)\n",
    "ax.set_xlim(0, 100)\n",
    "ax.set_ylim(0, 60)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add interpretation\n",
    "ax.text(60, 15, f'$y = {model_age.coef_[0]:.2f}x + {model_age.intercept_:.2f}$\\n$R^2 = {model_age.score(X_age, y_strength):.3f}$',\n",
    "        fontsize=12, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "- Each day adds approximately 0.39 MPa of strength\n",
    "- Base strength (intercept): ~16.5 MPa\n",
    "- $R^2 \\approx 0.87$ -- reasonable fit, but note that the relationship between age and concrete strength is actually nonlinear (logarithmic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "\n",
    "**Idea:** Use polynomial features to fit nonlinear relationships\n",
    "\n",
    "**Transform:**\n",
    "\n",
    "$$x \\rightarrow [x, x^2, x^3, \\ldots, x^d]$$\n",
    "\n",
    "Then apply linear regression: $\\hat{y} = w_0 + w_1 x + w_2 x^2 + \\cdots + w_d x^d$\n",
    "\n",
    "> **Key Insight: Warning**  \n",
    "> Higher degree $\\rightarrow$ more flexibility $\\rightarrow$ risk of overfitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#",
    "#",
    "#",
    " ",
    "[",
    "L",
    "I",
    "V",
    "E",
    "]",
    " ",
    "P",
    "o",
    "l",
    "y",
    "n",
    "o",
    "m",
    "i",
    "a",
    "l",
    " ",
    "R",
    "e",
    "g",
    "r",
    "e",
    "s",
    "s",
    "i",
    "o",
    "n",
    ":",
    " ",
    "D",
    "e",
    "g",
    "r",
    "e",
    "e",
    " ",
    "C",
    "o",
    "m",
    "p",
    "a",
    "r",
    "i",
    "s",
    "o",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Sample data\n",
    "np.random.seed(42)\n",
    "x_data = np.array([-2, -1, 0, 1, 2])\n",
    "y_data = np.array([3.8, 1.2, 0.1, 1.8, 6.2])\n",
    "X_poly_data = x_data.reshape(-1, 1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "x_fine = np.linspace(-2.5, 2.5, 200).reshape(-1, 1)\n",
    "\n",
    "degrees = [1, 2, 5]\n",
    "titles = ['Degree 1 (Linear)', 'Degree 2 (Quadratic)', 'Degree 5 (Overfit Risk)']\n",
    "\n",
    "for ax, deg, title in zip(axes, degrees, titles):\n",
    "    # Create and fit pipeline\n",
    "    pipe = make_pipeline(PolynomialFeatures(deg), LinearRegression())\n",
    "    pipe.fit(X_poly_data, y_data)\n",
    "    y_fine = pipe.predict(x_fine)\n",
    "    r2 = pipe.score(X_poly_data, y_data)\n",
    "\n",
    "    ax.scatter(x_data, y_data, c='indianred', s=80, edgecolors='k', zorder=3, label='Data')\n",
    "    ax.plot(x_fine, y_fine, 'steelblue', linewidth=2, label=f'Fit ($R^2$={r2:.3f})')\n",
    "    ax.set_xlabel('$x$', fontsize=12)\n",
    "    ax.set_ylabel('$y$', fontsize=12)\n",
    "    ax.set_title(title, fontsize=13)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.set_xlim(-3, 3)\n",
    "    ax.set_ylim(-2, 8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Features in Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Original data\n",
    "X_poly = np.array([[x] for x in range(10)])\n",
    "y_poly = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12])\n",
    "\n",
    "# Create polynomial regression pipeline\n",
    "# Degree 3: [1, x, x^2, x^3]\n",
    "poly_model = make_pipeline(\n",
    "    PolynomialFeatures(degree=3),\n",
    "    LinearRegression()\n",
    ")\n",
    "\n",
    "# Fit and predict\n",
    "poly_model.fit(X_poly, y_poly)\n",
    "y_pred_poly = poly_model.predict(X_poly)\n",
    "\n",
    "# Evaluate\n",
    "r2 = poly_model.score(X_poly, y_poly)\n",
    "print(f\"R^2 score: {r2:.3f}\")\n",
    "print(f\"\\nPipeline chains transformations automatically!\")\n",
    "print(f\"PolynomialFeatures(degree=3) transforms x -> [1, x, x^2, x^3]\")\n",
    "print(f\"Then LinearRegression fits the transformed features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization: Controlling Complexity\n",
    "\n",
    "**Problem:** Complex models overfit to training data\n",
    "\n",
    "**Solution:** Add penalty for large coefficients\n",
    "\n",
    "| | **Ridge Regression (L2)** | **Lasso Regression (L1)** |\n",
    "|---|---|---|\n",
    "| **Objective** | $\\text{minimize} \\sum (y_i - \\hat{y}_i)^2 + \\alpha \\sum w_j^2$ | $\\text{minimize} \\sum (y_i - \\hat{y}_i)^2 + \\alpha \\sum \\|w_j\\|$ |\n",
    "| **Effect** | Shrinks coefficients | Shrinks some to exactly zero |\n",
    "| **Features** | Keeps all features | Performs feature selection |\n",
    "| **Parameter** | $\\alpha$: regularization strength | $\\alpha$: regularization strength |\n",
    "\n",
    "**Key:** Larger $\\alpha$ $\\rightarrow$ stronger regularization $\\rightarrow$ simpler model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of Regularization on Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "alpha_range = np.logspace(-2, 2, 100)\n",
    "\n",
    "# Simulated coefficient paths\n",
    "ridge_w1 = 5 / (1 + 0.2 * alpha_range)\n",
    "ridge_w2 = 3 / (1 + 0.2 * alpha_range)\n",
    "lasso_w1 = np.maximum(0, 5 - 0.5 * alpha_range)\n",
    "\n",
    "ax.plot(alpha_range, ridge_w1, color='steelblue', linewidth=2.5, label='Ridge: $w_1$')\n",
    "ax.plot(alpha_range, ridge_w2, color='green', linewidth=2.5, label='Ridge: $w_2$')\n",
    "ax.plot(alpha_range, lasso_w1, color='indianred', linewidth=2.5, linestyle='--', label='Lasso: $w_1$')\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Regularization $\\\\alpha$', fontsize=13)\n",
    "ax.set_ylabel('Coefficient Value', fontsize=13)\n",
    "ax.set_title('Effect of Regularization on Coefficients', fontsize=14)\n",
    "ax.legend(fontsize=12)\n",
    "ax.set_ylim(0, 6)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Annotations\n",
    "ax.annotate('Lasso drives\\ncoefficient to 0', xy=(10, 0.05), fontsize=10,\n",
    "            xytext=(20, 1.5), arrowprops=dict(arrowstyle='->', color='indianred'),\n",
    "            color='indianred', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge and Lasso in Scikit-Learn\n",
    "\n",
    "> **Example: When to Use Which?**  \n",
    "> **Ridge:** When all features are potentially relevant  \n",
    "> **Lasso:** When you want automatic feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use iris regression task (predict petal width from other features)\n",
    "iris = load_iris()\n",
    "X_reg = iris.data[:, :3]\n",
    "y_reg = iris.data[:, 3]\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ridge Regression (L2)\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train_reg, y_train_reg)\n",
    "ridge_score = ridge.score(X_test_reg, y_test_reg)\n",
    "\n",
    "# Lasso Regression (L1)\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train_reg, y_train_reg)\n",
    "lasso_score = lasso.score(X_test_reg, y_test_reg)\n",
    "\n",
    "# Compare coefficients\n",
    "print(f\"Ridge R^2: {ridge_score:.3f}\")\n",
    "print(f\"Ridge coefficients: {ridge.coef_}\")\n",
    "print(f\"\\nLasso R^2: {lasso_score:.3f}\")\n",
    "print(f\"Lasso coefficients: {lasso.coef_}\")\n",
    "print(f\"Non-zero Lasso features: {np.sum(lasso.coef_ != 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-World Example: Bicycle Traffic Prediction\n",
    "\n",
    "**Problem:** Predict daily bicycle traffic on Seattle's Fremont Bridge\n",
    "\n",
    "**Features:**\n",
    "- Temperature, precipitation\n",
    "- Day of week, month\n",
    "- Holiday indicator\n",
    "- Hour of day (if hourly data)\n",
    "\n",
    "**Approach:**\n",
    "1. Feature engineering: add polynomial features for temperature\n",
    "2. Add interaction terms (e.g., temp x weekend)\n",
    "3. Use Ridge regression to prevent overfitting\n",
    "4. Validate with cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "**1. Machine Learning Fundamentals**\n",
    "- Supervised (classification, regression) vs Unsupervised (clustering, dim reduction)\n",
    "- Data-driven approach to building predictive models\n",
    "\n",
    "**2. Scikit-Learn Workflow**\n",
    "- Consistent API: `fit()`, `predict()`, `transform()`\n",
    "- Data representation: features matrix $\\mathbf{X}$, target vector $\\mathbf{y}$\n",
    "\n",
    "**3. Model Validation**\n",
    "- Never evaluate on training data\n",
    "- Use train-test split or cross-validation\n",
    "- Understand bias-variance tradeoff\n",
    "\n",
    "**4. Linear Regression**\n",
    "- Foundation for many ML algorithms\n",
    "- Polynomial features for nonlinearity\n",
    "- Regularization (Ridge/Lasso) prevents overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Civil Engineering Applications\n",
    "\n",
    "**Machine Learning is transforming civil engineering:**\n",
    "\n",
    "1. **Structural Health Monitoring**\n",
    "   - Classify damage types from sensor data\n",
    "   - Predict remaining service life\n",
    "\n",
    "2. **Material Science**\n",
    "   - Predict concrete/steel properties from composition\n",
    "   - Optimize mix designs\n",
    "\n",
    "3. **Traffic & Transportation**\n",
    "   - Traffic flow prediction and optimization\n",
    "   - Route planning and demand forecasting\n",
    "\n",
    "4. **Construction Management**\n",
    "   - Project cost and duration estimation\n",
    "   - Risk assessment and safety prediction\n",
    "\n",
    "5. **Environmental Engineering**\n",
    "   - Water quality prediction\n",
    "   - Climate impact assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps in Machine Learning\n",
    "\n",
    "**Coming in Week 7:**\n",
    "- **Naive Bayes:** Probabilistic classification\n",
    "- **Support Vector Machines:** Maximum-margin classifiers\n",
    "- **Decision Trees & Random Forests:** Ensemble methods\n",
    "- **Clustering:** K-Means, hierarchical clustering\n",
    "- **Dimensionality Reduction:** PCA deep dive\n",
    "\n",
    "**Practice Resources:**\n",
    "- **Scikit-Learn Documentation:** https://scikit-learn.org\n",
    "- **Kaggle:** Real-world datasets and competitions\n",
    "- **Course Notebooks:** Hands-on examples in repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Questions?\n",
    "\n",
    "**Dr. Eyuphan Koc**  \n",
    "eyuphan.koc@bogazici.edu.tr  \n",
    "\n",
    "*Office Hours: By appointment*\n",
    "\n",
    "**Next Lecture:** Advanced ML Algorithms (Naive Bayes, SVM, Random Forests)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}